{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of Experiment\n",
    "\n",
    "This jupyter notebook uses the Edge-IIoTset2023 public dataset (available at https://doi.org/10.1109/ACCESS.2022.3165809) to explore the use of Ensemble Learning methods as a means to improve predictive accuracy for anomaly detection.\n",
    "\n",
    "- Hardware environment: Intel i7-1017U processor (6 cores 12 threads), 64GB RAM, 1TB SSD storage\n",
    "- Software environment: Windows 11 Professional, Anaconda Navigator, scikit-learn version 1.3.0\n",
    "- This notebook takes ~3 hours to run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Works for subsequent projects\n",
    "- perform  additional feature selection, add correlation heatmap, remove features with low correlation\n",
    "- add some persistence with pickle, save the reduced dataset as a local CSV file to speed up iteration\n",
    "- code cleanup: y_test_label should be y_test to make the variable naming more consistent with sample code\n",
    "- consider using \"Beautiful Soup\" package to analyze HTTP text data\n",
    "- use NLP for extracting text data from features like \"http_uri_query\" to analyze the contents of the text and extract meaningful information\n",
    "- This project only used 2 classes (normal vs attack), consider using multi-class classification so you can determine what specific type of attack (brute force, DoS, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Miscellaneous packages\n",
    "import time                                           #for calculating elapsed time for training tasks\n",
    "import os                                             #for checking if file exists\n",
    "import socket                                         #for getting FQDN of local machine\n",
    "import math                                           #square root function\n",
    "import sys\n",
    "\n",
    "# Packages from scikit-learn\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV       #for hyperparameter optimization\n",
    "from sklearn.model_selection import cross_val_score    #for cross fold validation\n",
    "from sklearn.metrics         import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.linear_model    import LogisticRegression\n",
    "from sklearn.svm             import SVC    \n",
    "from sklearn.neighbors       import KNeighborsClassifier\n",
    "from sklearn.ensemble        import RandomForestClassifier\n",
    "from sklearn.neural_network  import MLPClassifier\n",
    "from sklearn.naive_bayes     import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.ensemble        import BaggingClassifier, VotingClassifier, StackingClassifier, AdaBoostClassifier, GradientBoostingClassifier   #Packages for Ensemble Learning\n",
    "from sklearn.linear_model    import LogisticRegression          #used by stacking models\n",
    "from sklearn.tree            import DecisionTreeClassifier      #used by stacking models\n",
    "\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler  #may need to install with: conda install -c conda-forge imbalanced-learn\n",
    "from imblearn.over_sampling  import SMOTE               #may need to install with: conda install -c conda-forge imbalanced-learn\n",
    "import xgboost as xgb                                   #eXtreme Gradient Booster, not part of sklearn, need to install with: pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to show missing values in dataset\n",
    "\n",
    "def get_type_missing(df):\n",
    "    df_types = pd.DataFrame()\n",
    "    df_types['data_type'] = df.dtypes\n",
    "    df_types['missing_values'] = df.isnull().sum()\n",
    "    return df_types.sort_values(by='missing_values', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a confusion matrix\n",
    "\n",
    "def visualize_confusion_matrix(y_test_label, y_pred):\n",
    "    #\n",
    "    ## Calculate accuracy\n",
    "    #accuracy = accuracy_score(y_test_label, y_pred)\n",
    "    #print(\"Accuracy:\", accuracy)\n",
    "    #\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test_label, y_pred)\n",
    "    #\n",
    "    # visualize confusion matrix with more detailed labels\n",
    "    # https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea\n",
    "    #\n",
    "    group_names = ['True Negative','False Positive','False Negative','True Positive']\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in cm.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in cm.flatten()/np.sum(cm)]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    plt.figure(figsize=(3.5, 2.0))  #default figsize is 6.4\" wide x 4.8\" tall, shrink to 3.5\" wide 2.0\" tall\n",
    "    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', cbar=False)\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "    # use the .ravel function to pull out TN,TP,FN,TP\n",
    "    # https://analytics4all.org/2020/05/07/python-confusion-matrix/\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    \n",
    "    # calculate different metrics\n",
    "    Accuracy = (( TP + TN) / ( TP + TN + FP + FN))\n",
    "    Sensitivity = TP / (TP + FN)\n",
    "    Specificity = TN / (TN + FP)\n",
    "    GeometricMean = math.sqrt(Sensitivity * Specificity)\n",
    "\n",
    "    # Precision is the ratio of true positive predictions to the total number of positive predictions made by the model\n",
    "    # average=binary for  binary classification models, average=micro for multiclass classification, average=weighted to match classification_report\n",
    "    Precision = precision_score(y_test_label, y_pred, average='weighted')  \n",
    "    \n",
    "    # Recall is the ratio of true positive predictions to the total number of actual positive instances in the data.\n",
    "    # average=binary for  binary classification models, average=micro for multiclass classification, average=weighted to match classification_report\n",
    "    Recall = recall_score(y_test_label, y_pred, average='weighted') \n",
    "    \n",
    "    # F1-score is a metric that considers both precision and recall, providing a balance between the two. \n",
    "    # average=binary for  binary classification models, average=micro for multiclass classification, average=weighted to match classification_report\n",
    "    F1 = f1_score(y_test_label, y_pred, average='weighted')\n",
    "    \n",
    "    # add details below graph to help interpret results\n",
    "    print('\\n\\n')\n",
    "    print('Confusion matrix\\n\\n', cm)\n",
    "    print('\\nTrue Negatives  (TN) = ', TN)\n",
    "    print('False Positives (FP) = ', FP)\n",
    "    print('False Negatives (FN) = ', FN)\n",
    "    print('True Positives  (TP) = ', TP)\n",
    "    print ('\\n')\n",
    "    print (\"Accuracy:       \", Accuracy)\n",
    "    print (\"Sensitivity:    \", Sensitivity)\n",
    "    print (\"Specificity:    \", Specificity)\n",
    "    print (\"Geometric Mean: \", GeometricMean)\n",
    "    print ('\\n')\n",
    "    print (\"Precision:       \", Precision)\n",
    "    print (\"Recall:          \", Recall)\n",
    "    print (\"f1-score:        \", F1)\n",
    "    \n",
    "    print('\\n------------------------------------------------\\n')\n",
    "    # We want TN and TP to be approximately equal, because this indicates the dataset is well balanced.\n",
    "    # If TN and TP are very different, it indicates imbalanced data, which can lead to low accuracy due to overfitting\n",
    "    #if (TN/TP*100 < 40 or TN/TP*100 > 60):   #we want TN and TP to be approximately 50%, if the values are below 40% or over 60%, generate a warning\n",
    "    #    print(\"WARNING: the confusion matrix shows that TN and TP are very imbalanced, may lead to low accuracy!\")\n",
    "    #\n",
    "    return cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to report on model accuracy (TP, FP, FN, FP), precision, recall, f1-score\n",
    "# this function does not provide anything additional to the results from the previous function\n",
    "\n",
    "def model_classification_report(cm, y_test_label, y_pred):\n",
    "    report = classification_report(y_test_label, y_pred, digits=4)\n",
    "    print('\\n')\n",
    "    print(\"Classification Report: \\n\", report)\n",
    "    print('\\n\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to show elapsed time for running notebook\n",
    "\n",
    "# start a timer so we can calculate the total runtime of this notebook\n",
    "notebook_start_time = time.time()  #seconds since epoch\n",
    "\n",
    "def show_elapsed_time():\n",
    "    #\n",
    "    # Get the current time as a struct_time object\n",
    "    current_time_struct = time.localtime()                             \n",
    "    \n",
    "    # Format the struct_time as a string (yyyy-mm-dd HH:MM:SS format)\n",
    "    current_time_str = time.strftime(\"%Y-%m-%d %H:%M:%S\", current_time_struct)  \n",
    "    \n",
    "    # Display the current time in HH:MM:SS format\n",
    "    print(\"Current Time:\", current_time_str)      \n",
    "    \n",
    "    # show a running total of elapsed time for the entire notebook\n",
    "    notebook_end_time = time.time()  #seconds since epoch\n",
    "    print(f\"The entire notebook runtime so far is {(notebook_end_time-notebook_start_time)/60:.0f} minutes\")\n",
    "\n",
    "show_elapsed_time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize variables to avoid undef errors\n",
    "\n",
    "accuracy_lr_undersampled_unoptimized  = 0\n",
    "accuracy_lr_undersampled_optimized    = 0\n",
    "accuracy_dt_undersampled_unoptimized  = 0\n",
    "accuracy_dt_undersampled_optimized    = 0\n",
    "accuracy_ds_undersampled_unoptimized  = 0\n",
    "accuracy_ds_undersampled_optimized    = 0\n",
    "accuracy_rf_undersampled_unoptimized  = 0\n",
    "accuracy_rf_undersampled_optimized    = 0\n",
    "accuracy_nb_undersampled_unoptimized  = 0\n",
    "accuracy_nb_undersampled_optimized    = 0\n",
    "accuracy_svm_undersampled_unoptimized = 0\n",
    "accuracy_svm_undersampled_optimized   = 0\n",
    "accuracy_knn_undersampled_unoptimized = 0\n",
    "accuracy_knn_undersampled_optimized   = 0\n",
    "accuracy_mlp_undersampled_unoptimized = 0\n",
    "accuracy_mlp_undersampled_optimized   = 0\n",
    "accuracy_gb_undersampled_unoptimized  = 0\n",
    "accuracy_gb_undersampled_optimized    = 0\n",
    "accuracy_xgb_undersampled_unoptimized = 0\n",
    "accuracy_xgb_undersampled_optimized   = 0\n",
    "\n",
    "best_params_lr                        = \"\"\n",
    "best_params_dt                        = \"\"\n",
    "best_params_ds                        = \"\"\n",
    "best_params_rf                        = \"\"\n",
    "best_params_nb                        = \"\"\n",
    "best_params_svm                       = \"\"\n",
    "best_params_knn                       = \"\"\n",
    "best_params_mlp                       = \"\"\n",
    "best_params_gb                        = \"\"\n",
    "best_params_xgb                       = \"\"\n",
    "\n",
    "accuracy_ensemble_voting              = 0\n",
    "accuracy_ensemble_stacking            = 0\n",
    "accuracy_ensemble_boosting            = 0\n",
    "accuracy_ensemble_bagging             = 0\n",
    "\n",
    "cv_count                              = 10  #number of cross-validation folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define CSV source file\n",
    "\n",
    "filename = 'DNN-EdgeIIoT-dataset.csv'\n",
    "LAN_location = 'http://datasets.nyx.local:80/datasets/Edge-IIoTset2023/Selected_dataset_for_ML_and_DL'  #high speed local copy on LAN\n",
    "WAN_location = 'http://datasets.nyx.ca:8081/datasets/Edge-IIoTset2023/Selected_dataset_for_ML_and_DL'   #accessible to entire internet\n",
    "\n",
    "\n",
    "\n",
    "# Get the FQDN of the local machine\n",
    "fqdn = socket.getfqdn()\n",
    "ipv4_address = socket.gethostbyname(socket.gethostname())\n",
    "print(f\"Fully Qualified Domain Name (FQDN):{fqdn}, IPv4 address:{ipv4_address}\")\n",
    "if ( \"nyx.local\" in fqdn ):\n",
    "    # If inside the LAN, grab the local copy of the dataset\n",
    "    print(f\"Detected Fully Qualified Domain Name of {fqdn}, dataset source is:\\n{LAN_location}/{filename}\")\n",
    "    dataset = f\"{LAN_location}/{filename}\"\n",
    "else:\n",
    "    # If not inside the LAN, grab the dataset from an internet-accessible URL\n",
    "    print(f\"Detected Fully Qualified Domain Name of {fqdn}, dataset source is:\\n{WAN_location}/{filename}\")\n",
    "    dataset = f\"{WAN_location}/{filename}\"\n",
    "\n",
    "    \n",
    "print(f\"Loading dataset from {dataset}\")\n",
    "df = pd.read_csv(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Dropping rows from the dataset during debugging to speed up this notebook - turn this off when finished debugging!\")\n",
    "\n",
    "# # cut dataset in half if > 2 million rows\n",
    "# if ( len(df) > 2000000):\n",
    "#     print(f\"Original size of dataset is\", len(df), \" rows\")\n",
    "#     df.drop(df.index[::2], inplace=True)\n",
    "#     print(f\"Dataset size after dropping all the even-numbered rows is\", len(df), \" rows\")\n",
    "\n",
    "# # cut dataset in half if > 1 million rows\n",
    "# if ( len(df) > 1000000):\n",
    "#     print(f\"Original size of dataset is\", len(df), \" rows\")\n",
    "#     df.drop(df.index[::2], inplace=True)\n",
    "#     print(f\"Dataset size after dropping all the even-numbered rows is\", len(df), \" rows\")\n",
    "\n",
    "# # cut dataset in half if > 0.5 million rows\n",
    "# if ( len(df) > 500000):\n",
    "#     print(f\"Original size of dataset is\", len(df), \" rows\")\n",
    "#     df.drop(df.index[::2], inplace=True)\n",
    "#     print(f\"Dataset size after dropping all the even-numbered rows is\", len(df), \" rows\")\n",
    "\n",
    "# # cut dataset in half if > 0.5 million rows\n",
    "# if ( len(df) > 500000):\n",
    "#     print(f\"Original size of dataset is\", len(df), \" rows\")\n",
    "#     df.drop(df.index[::2], inplace=True)\n",
    "#     print(f\"Dataset size after dropping all the even-numbered rows is\", len(df), \" rows\")\n",
    "\n",
    "# # cut dataset in half if > 250,000 rows\n",
    "# if ( len(df) > 250000):\n",
    "#     print(f\"Original size of dataset is\", len(df), \" rows\")\n",
    "#     df.drop(df.index[::2], inplace=True)\n",
    "#     print(f\"Dataset size after dropping all the even-numbered rows is\", len(df), \" rows\")\n",
    "\n",
    "\n",
    "# # cut dataset in half if > 100,000 rows\n",
    "# if ( len(df) > 100000):\n",
    "#     print(f\"Original size of dataset is\", len(df), \" rows\")\n",
    "#     df.drop(df.index[::2], inplace=True)\n",
    "#     print(f\"Dataset size after dropping all the even-numbered rows is\", len(df), \" rows\")\n",
    "\n",
    "\n",
    "# # cut dataset in half if > 50,000 rows\n",
    "# if ( len(df) > 50000):\n",
    "#     print(f\"Original size of dataset is\", len(df), \" rows\")\n",
    "#     df.drop(df.index[::2], inplace=True)\n",
    "#     print(f\"Dataset size after dropping all the even-numbered rows is\", len(df), \" rows\")\n",
    "\n",
    "\n",
    "# # cut dataset in half if > 25,000 rows\n",
    "# if ( len(df) > 25000):\n",
    "#     print(f\"Original size of dataset is\", len(df), \" rows\")\n",
    "#     df.drop(df.index[::2], inplace=True)\n",
    "#     print(f\"Dataset size after dropping all the even-numbered rows is\", len(df), \" rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view dimensions of dataset (rows and columns)\n",
    "print (\"Rows,columns in dataset:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a quick look at the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all the data rather than just a portion\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for any missing values in dataset\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for any missing datatypes\n",
    "get_type_missing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at all the datatypes of that are objects, in case any can be converted to integers\n",
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the values in all of the features\n",
    "\n",
    "feature_names = df.columns.tolist()\n",
    "\n",
    "for feature_name in feature_names:\n",
    "    if feature_name in df.columns:\n",
    "        print('\\n')\n",
    "        print(f\"------------------\")\n",
    "        print(f\"{feature_name}\")\n",
    "        print(f\"------------------\")\n",
    "        print(df[feature_name].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view dimensions of dataset (rows and columns)\n",
    "print (\"Rows,columns in dataset:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix up feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['frame.time'].value_counts().head())\n",
    "\n",
    "print(\"\\nNull Values:\")\n",
    "print(df['frame.time'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to datetime\n",
    "def convert_to_datetime(value):\n",
    "    try:\n",
    "         return pd.to_datetime(value)\n",
    "    except:\n",
    "        return np.nan\n",
    "       \n",
    "# skip the time-consuming conversion because we drop this feature later\n",
    "#df['frame.time'] = df['frame.time'].apply(convert_to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating IP address\n",
    "\n",
    "print(df['ip.src_host'].value_counts().head())\n",
    "print('_________________________________________________________')\n",
    "print(df['ip.dst_host'].value_counts().head())\n",
    "print('_________________________________________________________')\n",
    "print(df['arp.src.proto_ipv4'].value_counts().head())\n",
    "print('_________________________________________________________')\n",
    "print(df['arp.dst.proto_ipv4'].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for fun explore these values in the http.file_data column\n",
    "#df[df['Attack_label'] == 1]['http.file_data'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mqtt.topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mqtt.protoname'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dns.qry.name.len'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['http.request.method'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- exploring the target dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many 0 (normal) and 1 (attack) values do we have?\n",
    "df['Attack_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "sns.countplot(data=df, x='Attack_label', hue='Attack_type', edgecolor='black', linewidth=1)\n",
    "plt.title('Attack Label vs Attack Type', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.pie(df, names='Attack_label', title='Distribution of Attack Labels')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(df, names='Attack_type', title='Distribution of Attack Type')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- class imbalance issue - this can cause the machine learning model to result in biased results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop features \n",
    "Now using our domain knowledge we will only select useful features from our dataset and drop the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view dimensions of dataset (rows and columns)\n",
    "print (\"Rows,columns in dataset:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying columns that are entirely NaN (empty) or have all zero values\n",
    "empty_or_zero_columns = df.columns[(df.isnull().all())\n",
    "| (df == 0).all()   | (df == 1).all() | (df == 1.0).all()\n",
    "| (df == 0.0).all() | (df == 2).all() | (df == 2.0).all()]\n",
    "\n",
    "# Displaying the identified columns\n",
    "empty_features = empty_or_zero_columns.tolist()\n",
    "\n",
    "print(\"These columns are all empty features:\")\n",
    "print(empty_features)\n",
    "\n",
    "\n",
    "for feature in empty_features:\n",
    "  if feature in df.columns:\n",
    "    df.drop(feature, axis=1, inplace=True)\n",
    "    print(\"Dropping empty feature:\", feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the columns to confirm the features have been dropped\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view dimensions of dataset (rows and columns)\n",
    "print (\"Rows,columns in dataset:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop these features\n",
    "\n",
    "feature_names = [\"frame.time\", \"ip.src_host\", \"ip.dst_host\", \"arp.src.proto_ipv4\",\"arp.dst.proto_ipv4\",\n",
    "                \"http.file_data\",\"http.request.full_uri\",\"icmp.transmit_timestamp\",\n",
    "                \"http.request.uri.query\", \"tcp.options\",\"tcp.payload\",\"tcp.srcport\",\n",
    "                \"tcp.dstport\", \"udp.port\", \"mqtt.msg\", \"icmp.unused\", \"http.tls_port\", 'dns.qry.type', \n",
    "                'dns.retransmit_request_in', \"mqtt.msg_decoded_as\", \"mbtcp.trans_id\", \"mbtcp.unit_id\", \"http.request.method\", \"http.referer\", \n",
    "                \"http.request.version\", \"dns.qry.name.len\", \"mqtt.conack.flags\", \"mqtt.protoname\", \"mqtt.topic\"]\n",
    "\n",
    "# potential_drop_list = ['arp.opcode']\n",
    "\n",
    "for feature_name in feature_names:\n",
    "  if feature_name in df.columns:\n",
    "    df.drop(feature_name, axis=1, inplace=True)\n",
    "    print(\"Dropping feature:\", feature_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view dimensions of dataset (rows and columns)\n",
    "print (\"Rows,columns in dataset after dropping features:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df[df['tcp.flags.ack'] == 1]['Attack_label'].value_counts(normalize=True))\n",
    "# print(df[df['tcp.flags.ack'] == 0]['Attack_label'].value_counts(normalize=True))\n",
    "\n",
    "df['Attack_label'].groupby(df['tcp.flags.ack']).value_counts(normalize=True)\n",
    "# hence we group by is prefered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view dimensions of dataset (rows and columns)\n",
    "print (\"Rows,columns in dataset:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Problem: if we use a machine learning model to predict the Attack label, it could predict it as 0.1, 0.2 or 0.99 which is not a valid Attack label\n",
    "- Solution: Label Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![one hot encoding](https://www.blog.trainindata.com/wp-content/uploads/2023/10/cover-2.gif \"one hot encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['Attack_label'] = le.fit_transform(df['Attack_label'])\n",
    "\n",
    "df['Attack_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final column in the dataset is Attack_type, and will contain one of these values:\n",
    "\n",
    "# Display unique values in the \"Attack_type\" column\n",
    "unique_attack_types = df['Attack_type'].unique()\n",
    "print(\"Unique Attack Types:\")\n",
    "print(unique_attack_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate X and y variables (independent and dependent variables)\n",
    "\n",
    "X = df.drop(['Attack_label', 'Attack_type'], axis=1)\n",
    "y_label = df['Attack_label']\n",
    "y_type = df['Attack_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train / test / split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_label, y_test_label = train_test_split(X, y_label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class balancing with random undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RandomUnderSampler\n",
    "rus = RandomUnderSampler(sampling_strategy=1, random_state=42)\n",
    "\n",
    "# Apply Random Under Sampling\n",
    "X_train_resampled, y_train_label_resampled = rus.fit_resample(X_train, y_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you wanted to balance the classes with SMOTE instead, sample code shown below:\n",
    "\n",
    "## Create an instance of the SMOTE class\n",
    "#smote = SMOTE(sampling_strategy='auto')\n",
    "\n",
    "## Apply SMOTE to the training data\n",
    "#X_train_resampled, y_train_type_resampled = smote.fit_resample(X_train, y_train_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class balance before resampling\")\n",
    "print(y_train_label.value_counts())\n",
    "print('\\n')\n",
    "print(\"Class balance after resampling\")\n",
    "print(y_train_label_resampled.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUG ALERT - classes are not balanced!  \n",
    "# I think this is because we are using label encoding of the Attack_type feature, not the Attack_label feature\n",
    "\n",
    "# confirm the classes are balanced\n",
    "\n",
    "# the final column in the dataframe is named \"Attack_label\", and will be 0 f the data is normal,\n",
    "# or 1 if the data indicates an attack.\n",
    "\n",
    "# Figure out how many rows of each class exist in the dataframe\n",
    "normal_class = (df[(df['Attack_label'] == 0)])\n",
    "print(\"Number of rows in   normal class:\", (len(normal_class)) )\n",
    "\n",
    "abnormal_class = (df[(df['Attack_label'] == 1)])\n",
    "print(f\"Number of rows in abnormal class:\", (len(abnormal_class)) )\n",
    "\n",
    "total_rows = len(abnormal_class) + len(normal_class)\n",
    "print(f\"Total Number of rows (normal+abnormal): {total_rows}\" )\n",
    "\n",
    "balance = len(abnormal_class) / total_rows * 100\n",
    "balance = round(balance,2)\n",
    "\n",
    "print(f\"Percentage of abnormal class in dataset (abnormal/total*100): {balance}%\")\n",
    "if (balance  < 10): print(\"This dataset is very imbalanced, please beware of overfitting.\")\n",
    "if (balance == 50): print(\"This dataset is perfectly balanced.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_data_scaled = \"yes\"   #yes|no flag to turn feature scaling on|off to see if it changes prediction accuracy\n",
    "\n",
    "\n",
    "if (is_data_scaled == \"yes\"):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "    X_test_scaled = scaler.transform(X_test)  # Only transform the test set, don't fit\n",
    "    # Save the values under original names so we can use consistent names in subsequent sections\n",
    "    X_train_resampled = X_train_scaled\n",
    "    X_test = X_test_scaled\n",
    "else:\n",
    "    print(f\"WARNING: dataset is not being scaled, so the results may be skewed due to data distribution!\")\n",
    "\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce dataset size to speed up analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The current size of the dataset is:\")\n",
    "print(f\"   X_train_resampled       contains\", len(X_train_resampled), \"rows\")\n",
    "print(f\"   y_train_label_resampled contains\", len(y_train_label_resampled), \"rows\")\n",
    "print(f\"\\nThe objective of this section is to see if we can speed up the training process by reducing the size of the dataset, but not losing too much accuracy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of fractions to keep\n",
    "fractions_to_keep = [0.01, 0.02, 0.05, 0.10, 0.25, 0.50, 0.75, 1.0]\n",
    "\n",
    "#initialize variables\n",
    "best_accuracy = 0\n",
    "best_fraction_to_keep = 0\n",
    "\n",
    "\n",
    "# Iterate through different fractions\n",
    "for fraction_to_keep in fractions_to_keep:\n",
    "    # Randomly subsample the training set\n",
    "    num_samples_to_keep = int(len(X_train_resampled) * fraction_to_keep)\n",
    "    random_indices = np.random.choice(len(X_train_resampled), num_samples_to_keep, replace=False)\n",
    "\n",
    "    X_train_subsampled = X_train_resampled[random_indices]\n",
    "    y_train_subsampled = y_train_label_resampled.iloc[random_indices]   #use .iloc becaue y_train_label_resampled is a 1-dimensional array\n",
    "\n",
    "    # Train your model on the subsampled data\n",
    "    clf = LogisticRegression(max_iter=800, random_state=42)\n",
    "    clf.fit(X_train_subsampled, y_train_subsampled)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Evaluate accuracy on the test set\n",
    "    accuracy = accuracy_score(y_test_label, y_pred)\n",
    "    print(f\"Accuracy on the test set (fraction_to_keep={fraction_to_keep:.4f}): {accuracy:.4f}\")\n",
    "    \n",
    "    # Save the accuracy levels for later comparison\n",
    "    if fraction_to_keep == 0.01: accuracy_001 = accuracy\n",
    "    if fraction_to_keep == 0.02: accuracy_002 = accuracy\n",
    "    if fraction_to_keep == 0.05: accuracy_005 = accuracy\n",
    "    if fraction_to_keep == 0.10: accuracy_010 = accuracy\n",
    "    if fraction_to_keep == 0.25: accuracy_025 = accuracy\n",
    "    if fraction_to_keep == 0.50: accuracy_050 = accuracy\n",
    "    if fraction_to_keep == 0.75: accuracy_075 = accuracy\n",
    "    if fraction_to_keep == 1.0:  accuracy_100 = accuracy\n",
    "    \n",
    "    # keep track of the best accuracy\n",
    "    if accuracy > best_accuracy: \n",
    "        best_accuracy = accuracy\n",
    "        best_fraction_to_keep = fraction_to_keep\n",
    "\n",
    "\n",
    "print(f\"The highest accuracy is {best_accuracy:.4f} using the {best_fraction_to_keep} fraction of the dataset\")    \n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results from the previous cell\n",
    "\n",
    "# Data extracted from the image\n",
    "data = {\n",
    "    'fraction_to_keep': [0.010, 0.020, 0.050, 0.100, 0.250, 0.500, 0.750, 1.000],\n",
    "    'accuracy': [accuracy_001, accuracy_002, accuracy_005, accuracy_010, accuracy_025, accuracy_050, accuracy_075, accuracy_100]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['fraction_to_keep'], df['accuracy'], marker='o')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Accuracy on the Test Set by Fraction of Data Kept')\n",
    "plt.xlabel('Fraction of Data Kept')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Adding text for each data point\n",
    "for i in range(len(df)):\n",
    "    plt.text(df['fraction_to_keep'][i], df['accuracy'][i], f\"{df['fraction_to_keep'][i]*100}%\", ha='right')\n",
    "\n",
    "# Adding grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the figure with texts\n",
    "fig_path_with_text = 'accuracy_vs_data_fraction_with_text.png'\n",
    "plt.savefig(fig_path_with_text)\n",
    "\n",
    "# Show the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will programnmatically determine the best_fraction_to_keep, by sacrificing some (small) amount of accuracy for speed.\n",
    "# Exactly how small?  Let's go with an acceptable loss of 1% of accuracy for better speed.\n",
    "\n",
    "acceptable_loss_of_accuracy = 0.0100  # 0.01*100= 1%  Tweak this value depending on how much accuracy you are willing to sacrifice\n",
    "\n",
    "if ((best_accuracy - acceptable_loss_of_accuracy) <= accuracy_100):\n",
    "    print(f\"Using 100% of the dataset gives {accuracy_100*100:.2f}% accuracy, which is an acceptable trade-off between accuracy and speed.\")\n",
    "    best_fraction_to_keep = 1.0\n",
    "\n",
    "if ((best_accuracy - acceptable_loss_of_accuracy) <= accuracy_075):\n",
    "    print(f\"Using  75% of the dataset gives {accuracy_075*100:.2f}% accuracy, which is an acceptable trade-off between accuracy and speed.\")\n",
    "    best_fraction_to_keep = 0.75\n",
    "\n",
    "if ((best_accuracy - acceptable_loss_of_accuracy) <= accuracy_050):\n",
    "    print(f\"Using  50% of the dataset gives {accuracy_050*100:.2f}% accuracy, which is an acceptable trade-off between accuracy and speed.\")\n",
    "    best_fraction_to_keep = 0.50\n",
    "\n",
    "if ((best_accuracy - acceptable_loss_of_accuracy) <= accuracy_025):\n",
    "    print(f\"Using  25% of the dataset gives {accuracy_025*100:.2f}% accuracy, which is an acceptable trade-off between accuracy and speed.\")\n",
    "    best_fraction_to_keep = 0.25\n",
    "\n",
    "if ((best_accuracy - acceptable_loss_of_accuracy) <= accuracy_010):\n",
    "    print(f\"Using  10% of the dataset gives {accuracy_010*100:.2f}% accuracy, which is an acceptable trade-off between accuracy and speed.\")\n",
    "    best_fraction_to_keep = 0.10\n",
    "\n",
    "if ((best_accuracy - acceptable_loss_of_accuracy) <= accuracy_005):\n",
    "    print(f\"Using   5% of the dataset gives {accuracy_005*100:.2f}% accuracy, which is an acceptable trade-off between accuracy and speed.\")\n",
    "    best_fraction_to_keep = 0.05\n",
    "\n",
    "if ((best_accuracy - acceptable_loss_of_accuracy) <= accuracy_002):\n",
    "    print(f\"Using   2% of the dataset gives {accuracy_002*100:.2f}% accuracy, which is an acceptable trade-off between accuracy and speed.\")\n",
    "    best_fraction_to_keep = 0.02\n",
    "    \n",
    "if ((best_accuracy - acceptable_loss_of_accuracy) <= accuracy_001):\n",
    "    print(f\"Using   1% of the dataset gives {accuracy_001*100:.2f}% accuracy, which is an acceptable trade-off between accuracy and speed.\")\n",
    "    best_fraction_to_keep = 0.01\n",
    "\n",
    "print(f\"\\nBased on the above calculations, we will keep {best_fraction_to_keep*100:.0f}% of the dataset, which will still provide acceptable accuracy.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the accuracy calculations in the previous cell, decide how much of the dataset to keep\n",
    "fraction_to_keep = best_fraction_to_keep\n",
    "\n",
    "# Randomly subsample the training set\n",
    "num_samples_to_keep = int(len(X_train_resampled) * fraction_to_keep)\n",
    "random_indices = np.random.choice(len(X_train_resampled), num_samples_to_keep, replace=False)\n",
    "\n",
    "#save the sub-sampled data to temporary variable names\n",
    "X_train_subsampled = X_train_resampled[random_indices]\n",
    "y_train_subsampled = y_train_label_resampled.iloc[random_indices]   #use .iloc becaue y_train_label_resampled is a 1-dimensional array\n",
    "\n",
    "#save the sub-sampled data back to the original variable names that are used in subsequent sections\n",
    "X_train_resampled = X_train_subsampled\n",
    "y_train_label_resampled = y_train_subsampled\n",
    "\n",
    "\n",
    "print(f\"After downsampling the data without losing too much accuracy, the new size of the dataset is:\")\n",
    "print(f\"   X_train_resampled       contains\", len(X_train_resampled), \"rows\")\n",
    "print(f\"   y_train_label_resampled contains\", len(y_train_label_resampled), \"rows\")\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training with traditional classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the LogisticRegression model\n",
    "clf = LogisticRegression()\n",
    "\n",
    "default_params = clf.get_params()\n",
    "print(f\"Training model with default hyperparameters of: {default_params}\")\n",
    "        \n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = clf.score(X_test, y_test_label)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# save accuracy for later comparison\n",
    "accuracy_lr_undersampled_unoptimized = accuracy\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "# We want to see approximately equal results from TN and TP \n",
    "cm = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the  model\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [1, 10, 100],\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'max_iter': [100, 200, 300],\n",
    "    'random_state': [42]                 #for reproducible results\n",
    "}\n",
    "\n",
    "# Create an instance of GridSearchCV\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=cv_count, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "best_scores = grid_search.best_score_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Scores:\", best_scores)\n",
    "\n",
    "# Create a new instance of the model with the best hyperparameters\n",
    "clf = LogisticRegression(**best_params)\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# final cross validation\n",
    "cross_val_score_result = cross_val_score(clf, X_train_resampled, y_train_label_resampled, cv=cv_count)\n",
    "print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "lr_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "lr_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "# Evaluate the model\n",
    "Accuracy = accuracy_score(y_test_label, y_pred)\n",
    "print(\"Accuracy:\", Accuracy)\n",
    "\n",
    "# save best parameters for later comparison\n",
    "best_params_lr = best_params\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "accuracy_lr_undersampled_optimized      = Accuracy\n",
    "sensitivity_lr_undersampled_optimized   = Sensitivity\n",
    "specificity_lr_undersampled_optimized   = Specificity\n",
    "geometricmean_lr_undersampled_optimized = GeometricMean\n",
    "precision_lr_undersampled_optimized     = Precision\n",
    "recall_lr_undersampled_optimized        = Recall\n",
    "f1_lr_undersampled_optimized            = F1\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the DecisionTreeClassifier model\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "default_params = clf.get_params()\n",
    "print(f\"Training model with default hyperparameters of: {default_params}\")\n",
    "        \n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = clf.score(X_test, y_test_label)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# save accuracy for later comparison\n",
    "accuracy_dt_undersampled_unoptimized = accuracy\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DT hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the DecisionTreeClassifier model\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'random_state': [42]                 #for reproducible results\n",
    "}\n",
    "\n",
    "# Create an instance of GridSearchCV\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=cv_count,n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "best_scores = grid_search.best_score_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Scores:\", best_scores)\n",
    "\n",
    "# Create a new instance of the model with the best hyperparameters\n",
    "clf = DecisionTreeClassifier(**best_params)\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# final cross validation\n",
    "cross_val_score_result = cross_val_score(clf, X_train_resampled, y_train_label_resampled, cv=cv_count)\n",
    "print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "dt_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "dt_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "# Evaluate the model\n",
    "Accuracy = accuracy_score(y_test_label, y_pred)\n",
    "print(\"Accuracy:\", Accuracy)\n",
    "\n",
    "# save best parameters for later comparison\n",
    "best_params_dt = best_params\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "accuracy_dt_undersampled_optimized      = Accuracy\n",
    "sensitivity_dt_undersampled_optimized   = Sensitivity\n",
    "specificity_dt_undersampled_optimized   = Specificity\n",
    "geometricmean_dt_undersampled_optimized = GeometricMean\n",
    "precision_dt_undersampled_optimized     = Precision\n",
    "recall_dt_undersampled_optimized        = Recall\n",
    "f1_dt_undersampled_optimized            = F1\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Stump \n",
    "Decision Stump is a special case of the Decision Tree classifier with max_depth=1\n",
    "\n",
    "The term \"Decision Stump\" typically refers to a decision tree with only one level, meaning it makes decisions based on a single feature. \n",
    "\n",
    "The main hyperparameters for a decision stump are usually the splitting criterion and the choice of the feature to split on. \n",
    "\n",
    "However, since decision stumps are simple, there might not be a lot of hyperparameters to optimize compared to more complex models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if there is any benefit to using Decision Stump instead of Decision Tree\n",
    "if (accuracy_ds_undersampled_unoptimized < accuracy_dt_undersampled_unoptimized):\n",
    "    print(f\"NOTE: Decision Stump is a special case of Decision Tree with max_depth=1, but does not seem to be beneficial for this dataset.\")\n",
    "    print(f\"Decision Tree accuracy is {accuracy_dt_undersampled_unoptimized*100:.2f}%, while Decision Stump accuracy is only {accuracy_ds_undersampled_unoptimized*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the DecisionTreeClassifier model\n",
    "clf = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "default_params = clf.get_params()\n",
    "print(f\"Training model with default hyperparameters of: {default_params}\")\n",
    "        \n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = clf.score(X_test, y_test_label)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "# save accuracy for later comparison\n",
    "accuracy_ds_undersampled_unoptimized = accuracy\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DS hyperparameter optimization\n",
    "\n",
    "\n",
    "Remember that decision stumps are very simple models, and hyperparameter tuning might not have as much impact as it would on more complex models. It's always a good practice to experiment and validate the performance on a validation set or through cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the DecisionTreeClassifier model with max_depth=1\n",
    "clf = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [1],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'random_state': [42]                 #for reproducible results\n",
    "}\n",
    "\n",
    "# Create an instance of GridSearchCV\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=cv_count,n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "best_scores = grid_search.best_score_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Scores:\", best_scores)\n",
    "\n",
    "# Create a new instance of the model with the best hyperparameters\n",
    "clf = DecisionTreeClassifier(**best_params)\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# final cross validation\n",
    "cross_val_score_result = cross_val_score(clf, X_train_resampled, y_train_label_resampled, cv=cv_count)\n",
    "print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "ds_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "ds_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "# Evaluate the model\n",
    "Accuracy = accuracy_score(y_test_label, y_pred)\n",
    "print(\"Accuracy:\", Accuracy)\n",
    "\n",
    "# save best parameters for later comparison\n",
    "best_params_ds = best_params\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "accuracy_ds_undersampled_optimized      = Accuracy\n",
    "sensitivity_ds_undersampled_optimized   = Sensitivity\n",
    "specificity_ds_undersampled_optimized   = Specificity\n",
    "geometricmean_ds_undersampled_optimized = GeometricMean\n",
    "precision_ds_undersampled_optimized     = Precision\n",
    "recall_ds_undersampled_optimized        = Recall\n",
    "f1_ds_undersampled_optimized            = F1\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if there is any benefit to using Decision Stump instead of Decision Tree\n",
    "if (accuracy_ds_undersampled_optimized < accuracy_dt_undersampled_optimized):\n",
    "    print(f\"NOTE: Decision Stump is a special case of Decision Tree with max_depth=1, but does not seem to be beneficial for this dataset.\")\n",
    "    print(f\"Decision Tree accuracy is {accuracy_dt_undersampled_optimized*100:.2f}%, while Decision Stump accuracy is only {accuracy_ds_undersampled_optimized*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the RandomForestClassifier model\n",
    "clf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "default_params = clf.get_params()\n",
    "print(f\"Training model with default hyperparameters of: {default_params}\")\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = clf.score(X_test, y_test_label)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# save accuracy for later comparison\n",
    "accuracy_rf_undersampled_unoptimized = accuracy\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the RandomForestClassifier model\n",
    "clf = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10],\n",
    "    'random_state': [42]                 #for reproducible results\n",
    "}\n",
    "\n",
    "# Create an instance of GridSearchCV\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=cv_count, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "best_scores = grid_search.best_score_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Scores:\", best_scores)\n",
    "\n",
    "# Create a new instance of the model with the best hyperparameters\n",
    "clf = RandomForestClassifier(**best_params)\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# final cross validation\n",
    "cross_val_score_result = cross_val_score(clf, X_train_resampled, y_train_label_resampled, cv=cv_count)\n",
    "print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "rf_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "rf_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "# Evaluate the model\n",
    "Accuracy = accuracy_score(y_test_label, y_pred)\n",
    "print(\"Accuracy:\", Accuracy)\n",
    "\n",
    "# save best parameters for later comparison\n",
    "best_params_rf = best_params\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "accuracy_rf_undersampled_optimized      = Accuracy\n",
    "sensitivity_rf_undersampled_optimized   = Sensitivity\n",
    "specificity_rf_undersampled_optimized   = Specificity\n",
    "geometricmean_rf_undersampled_optimized = GeometricMean\n",
    "precision_rf_undersampled_optimized     = Precision\n",
    "recall_rf_undersampled_optimized        = Recall\n",
    "f1_rf_undersampled_optimized            = F1\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "#clf = GaussianNB()    # suitable for continuous features\n",
    "#clf = MultinomialNB() # used for discrete data like word counts\n",
    "clf = BernoulliNB()    # suitable for binary data, gives best accuracy for this dataset\n",
    "\n",
    "default_params = clf.get_params()\n",
    "print(f\"Training model with default hyperparameters of: {default_params}\")\n",
    "        \n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = clf.score(X_test, y_test_label)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# save accuracy for later comparison\n",
    "accuracy_nb_undersampled_unoptimized = accuracy\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "clf = BernoulliNB()\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "# skip the sigmoid and poly kernels, rarely used\n",
    "param_grid = {'alpha': [0.1, 0.01, 0.001]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of GridSearchCV\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=cv_count, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "print(\"Performing GridSearchCV\")\n",
    "grid_search.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "best_scores = grid_search.best_score_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Scores:\", best_scores)\n",
    "\n",
    "# Create a new instance of model with the best hyperparameters\n",
    "clf = BernoulliNB(**best_params)\n",
    "\n",
    "# Fit the model to the training data\n",
    "print(\"Fitting the model\")\n",
    "clf.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# final cross validation\n",
    "cross_val_score_result = cross_val_score(clf, X_train_resampled, y_train_label_resampled, cv=cv_count)\n",
    "print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "nb_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "nb_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "# Evaluate the model\n",
    "Accuracy = accuracy_score(y_test_label, y_pred)\n",
    "print(\"Accuracy:\", Accuracy)\n",
    "\n",
    "# save best parameters for later comparison\n",
    "best_params_nb = best_params\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "accuracy_nb_undersampled_optimized      = Accuracy\n",
    "sensitivity_nb_undersampled_optimized   = Sensitivity\n",
    "specificity_nb_undersampled_optimized   = Specificity\n",
    "geometricmean_nb_undersampled_optimized = GeometricMean\n",
    "precision_nb_undersampled_optimized     = Precision\n",
    "recall_nb_undersampled_optimized        = Recall\n",
    "f1_nb_undersampled_optimized            = F1\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "clf = SVC()\n",
    "\n",
    "default_params = clf.get_params()\n",
    "print(f\"Training model with default hyperparameters of: {default_params}\")\n",
    "        \n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = clf.score(X_test, y_test_label)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# save accuracy for later comparison\n",
    "accuracy_svm_undersampled_unoptimized = accuracy\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"WARNING: SVM hyperparameter optimization is very CPU-intensive, this will take some time...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "clf = SVC()\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "# skip the sigmoid and poly kernels, rarely used\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['rbf', 'linear'],\n",
    "    'probability': [True],               #probability=True is required for VotingClassifier\n",
    "    'random_state': [42]                 #for reproducible results\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of GridSearchCV\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=cv_count, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "print(\"Performing GridSearchCV\")\n",
    "grid_search.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "best_scores = grid_search.best_score_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Scores:\", best_scores)\n",
    "\n",
    "# Create a new instance of model with the best hyperparameters\n",
    "clf = SVC(**best_params)\n",
    "\n",
    "# Fit the model to the training data\n",
    "print(\"Fitting the model\")\n",
    "clf.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# final cross validation\n",
    "cross_val_score_result = cross_val_score(clf, X_train_resampled, y_train_label_resampled, cv=cv_count)\n",
    "print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "svm_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "svm_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "# Evaluate the model\n",
    "Accuracy = accuracy_score(y_test_label, y_pred)\n",
    "print(\"Accuracy:\", Accuracy)\n",
    "\n",
    "# save best parameters for later comparison\n",
    "best_params_svm = best_params\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "accuracy_svm_undersampled_optimized      = Accuracy\n",
    "sensitivity_svm_undersampled_optimized   = Sensitivity\n",
    "specificity_svm_undersampled_optimized   = Specificity\n",
    "geometricmean_svm_undersampled_optimized = GeometricMean\n",
    "precision_svm_undersampled_optimized     = Precision\n",
    "recall_svm_undersampled_optimized        = Recall\n",
    "f1_svm_undersampled_optimized            = F1\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model with the desired number of neighbors (you can adjust n_neighbors)\n",
    "clf = KNeighborsClassifier(n_neighbors=5)  # You can change the value of n_neighbors as needed\n",
    "\n",
    "default_params = clf.get_params()\n",
    "print(f\"Training model with default hyperparameters of: {default_params}\")\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = clf.score(X_test, y_test_label)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# save accuracy for later comparison\n",
    "accuracy_knn_undersampled_unoptimized = accuracy\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "clf = KNeighborsClassifier()\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'n_neighbors': [5,10,15,20,30],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of GridSearchCV\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=cv_count, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "best_scores = grid_search.best_score_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Scores:\", best_scores)\n",
    "\n",
    "# Create a new instance of the model with the best hyperparameters\n",
    "clf = KNeighborsClassifier(**best_params)\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# final cross validation\n",
    "cross_val_score_result = cross_val_score(clf, X_train_resampled, y_train_label_resampled, cv=cv_count)\n",
    "print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "knn_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "knn_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "# Evaluate the model\n",
    "Accuracy = accuracy_score(y_test_label, y_pred)\n",
    "print(\"Accuracy:\", Accuracy)\n",
    "\n",
    "# save best parameters for later comparison\n",
    "best_params_knn = best_params\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "accuracy_knn_undersampled_optimized      = Accuracy\n",
    "sensitivity_knn_undersampled_optimized   = Sensitivity\n",
    "specificity_knn_undersampled_optimized   = Specificity\n",
    "geometricmean_knn_undersampled_optimized = GeometricMean\n",
    "precision_knn_undersampled_optimized     = Precision\n",
    "recall_knn_undersampled_optimized        = Recall\n",
    "f1_knn_undersampled_optimized            = F1\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Multi-Layer Perceptron deep neural network classifier\n",
    "\n",
    "MLPClassifier is a class in scikit-learn that represents a Multi-layer Perceptron (MLP) classifier, which is a type of artificial neural network. \n",
    "\n",
    "An MLP is a feedforward neural network that consists of multiple layers of nodes (neurons) and can learn complex patterns and relationships in data. \n",
    "\n",
    "The MLPClassifier is specifically designed for classification tasks.\n",
    "\n",
    "Example of all hyperparameters:\n",
    "    \n",
    "    mlp_classifier = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),  # Architecture of hidden layers\n",
    "    activation='relu',             # Activation function ('relu' is common)\n",
    "    solver='adam',                 # Optimization solver\n",
    "    alpha=0.0001,                  # L2 penalty (regularization)\n",
    "    batch_size='auto',             # Size of mini-batches ('auto' is adaptive)\n",
    "    learning_rate='constant',      # Learning rate schedule\n",
    "    learning_rate_init=0.001,      # Initial learning rate\n",
    "    max_iter=500,                  # Maximum number of iterations\n",
    "    shuffle=True,                  # Shuffle data in each iteration\n",
    "    random_state=42,               # Random seed for reproducibility\n",
    "    verbose=True                   # Print progress during training\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model \n",
    "clf = MLPClassifier(random_state=42)\n",
    "\n",
    "default_params = clf.get_params()\n",
    "print(f\"Training model with default hyperparameters of: {default_params}\")\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = clf.score(X_test, y_test_label)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# save accuracy for later comparison\n",
    "accuracy_mlp_undersampled_unoptimized = accuracy\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlp_classifier = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "\n",
    "\n",
    "# Create an instance of the model\n",
    "clf = MLPClassifier()\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(100, 50), (50, 25), (150, 100)],  #tuples for hidden layers\n",
    "    'max_iter': [300, 500, 800],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'random_state': [42]                 #for reproducible results\n",
    "}\n",
    "\n",
    "\n",
    "# other exaples to use in param_grid for testing\n",
    "#param_grid = {\n",
    "#    'hidden_layer_sizes': [(50, 25), (100, 50), (100, 100)],\n",
    "#    'activation': ['relu', 'tanh'],\n",
    "#    'alpha': [0.0001, 0.001, 0.01],\n",
    "#    'learning_rate': ['constant', 'adaptive'],\n",
    "#    'max_iter': [200, 300, 500],\n",
    "#}\n",
    "\n",
    "\n",
    "# Create an instance of GridSearchCV\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=cv_count, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "best_scores = grid_search.best_score_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Scores:\", best_scores)\n",
    "\n",
    "# Create a new instance of the model with the best hyperparameters\n",
    "clf = MLPClassifier(**best_params)\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# final cross validation\n",
    "cross_val_score_result = cross_val_score(clf, X_train_resampled, y_train_label_resampled, cv=cv_count)\n",
    "print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "mlp_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "mlp_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "# Evaluate the model\n",
    "Accuracy = accuracy_score(y_test_label, y_pred)\n",
    "print(\"Accuracy:\", Accuracy)\n",
    "\n",
    "# save best parameters for later comparison\n",
    "best_params_mlp = best_params\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "accuracy_mlp_undersampled_optimized      = Accuracy\n",
    "sensitivity_mlp_undersampled_optimized   = Sensitivity\n",
    "specificity_mlp_undersampled_optimized   = Specificity\n",
    "geometricmean_mlp_undersampled_optimized = GeometricMean\n",
    "precision_mlp_undersampled_optimized     = Precision\n",
    "recall_mlp_undersampled_optimized        = Recall\n",
    "f1_mlp_undersampled_optimized            = F1\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GB Gradient Boosting classifier\n",
    "\n",
    "XGboost and gradient boosting are both ensemble learning models\n",
    "Gradient boosting is built into sklearn, but xgboost needs to install its own package\n",
    "Let's start with gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "model = GradientBoostingClassifier(\n",
    "    n_estimators=100,           # Number of boosting stages (trees)\n",
    "    learning_rate=0.1,          # Step size shrinkage to prevent overfitting\n",
    "    max_depth=3,                # Maximum tree depth\n",
    "    random_state=42             # Seed for reproducibility\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "clf = GradientBoostingClassifier(random_state=42) \n",
    "\n",
    "default_params = clf.get_params()\n",
    "print(f\"Training model with default hyperparameters of: {default_params}\")\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = clf.score(X_test, y_test_label)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# save accuracy for later comparison\n",
    "accuracy_gb_undersampled_unoptimized = accuracy\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GB hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "default_params = clf.get_params()\n",
    "print(f\"Training model with default hyperparameters of: {default_params}\")\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 100, 300], \n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'random_state': [42]                 #for reproducible results\n",
    "}\n",
    "\n",
    "# Create an instance of GridSearchCV\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=cv_count, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "best_scores = grid_search.best_score_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Scores:\", best_scores)\n",
    "\n",
    "# Create a new instance of the model with the best hyperparameters\n",
    "clf = GradientBoostingClassifier(**best_params)\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# final cross validation\n",
    "cross_val_score_result = cross_val_score(clf, X_train_resampled, y_train_label_resampled, cv=cv_count)\n",
    "print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "gb_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "gb_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "# Evaluate the model\n",
    "Accuracy = accuracy_score(y_test_label, y_pred)\n",
    "print(\"Accuracy:\", Accuracy)\n",
    "\n",
    "# save best parameters for later comparison\n",
    "best_params_gb = best_params\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "accuracy_gb_undersampled_optimized      = Accuracy\n",
    "sensitivity_gb_undersampled_optimized   = Sensitivity\n",
    "specificity_gb_undersampled_optimized   = Specificity\n",
    "geometricmean_gb_undersampled_optimized = GeometricMean\n",
    "precision_gb_undersampled_optimized     = Precision\n",
    "recall_gb_undersampled_optimized        = Recall\n",
    "f1_gb_undersampled_optimized            = F1\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XBG XGBoost eXtreme Gradient Boosting classifier\n",
    "\n",
    "XGBoost (eXtreme Gradient Boosting) is a popular and powerful open-source machine learning library designed for speed and performance. \n",
    "\n",
    "It is an implementation of gradient boosting, a machine learning technique that builds a series of weak learners (typically decision trees) and combines their predictions to create a stronger, more accurate model.\n",
    "\n",
    "XGBoost is known for its efficiency, scalability, and ability to handle diverse types of data.\n",
    "\n",
    "XGBoost is not built into sklearn, you will need to install the package with: pip install xgboost\n",
    "\n",
    "\n",
    "In this example, the xgb.DMatrix is a data structure that XGBoost uses for efficient training. \n",
    "The params dictionary contains various hyperparameters for the XGBoost model, and xgb.train is used to train the model. \n",
    "Finally, predictions are made on the test set, and the accuracy is evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The xgboost library is not part of the default install of sklearn, check to see if xgboost library is installed\n",
    "if 'xgboost' in sys.modules:\n",
    "    print(f\"Confirmed xgboost library is installed\")\n",
    "else:\n",
    "    print(f\"ERROR: xgboost library is NOT installed, please install with: pip install xgboost\")\n",
    "\n",
    "\n",
    "\n",
    "# only run the rest of the cell if the xgboost library is installed\n",
    "if 'xgboost' in sys.modules:\n",
    "    # Convert data to DMatrix format (optimized data structure for XGBoost)\n",
    "    dtrain = xgb.DMatrix(X_train_resampled, label=y_train_label_resampled)\n",
    "    dtest  = xgb.DMatrix(X_test, label=y_test_label)\n",
    "\n",
    "    # Set parameters for XGBoost\n",
    "    params = {\n",
    "        'objective': 'multi:softmax',  # Multi-class classification\n",
    "        'num_class': 3,  # Number of classes\n",
    "        'max_depth': 3,\n",
    "        'eta': 0.1,\n",
    "        'eval_metric': 'merror'  # Mean classification error\n",
    "    }\n",
    "\n",
    "    # Train the XGBoost model\n",
    "    num_rounds = 100\n",
    "    xgb_model = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = xgb_model.predict(dtest)\n",
    "\n",
    "    # Convert predicted probabilities to class labels\n",
    "    y_pred = [int(round(pred)) for pred in y_pred]\n",
    "\n",
    "    # Evaluate the accuracy\n",
    "    accuracy = accuracy_score(y_test_label, y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    accuracy = clf.score(X_test, y_test_label)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    # save accuracy for later comparison\n",
    "    accuracy_xgb_undersampled_unoptimized = accuracy\n",
    "\n",
    "    # call previously defined function to create confusion matrix\n",
    "    cm = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "    # show a running total of elapsed time for the entire notebook\n",
    "    show_elapsed_time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The xgboost library is not part of the default install of sklearn, check to see if xgboost library is installed\n",
    "if 'xgboost' in sys.modules:\n",
    "    print(f\"Confirmed xgboost library is installed\")\n",
    "else:\n",
    "    print(f\"ERROR: xgboost library is NOT installed, please install with: pip install xgboost\")\n",
    "\n",
    "\n",
    "# only run the rest of the cell if the xgboost library is installed\n",
    "if 'xgboost' in sys.modules:\n",
    "\n",
    "    # Create an instance of the model\n",
    "    clf = xgb.XGBClassifier()\n",
    "\n",
    "    default_params = clf.get_params()\n",
    "    print(f\"Default hyperparameters are: {default_params}\")\n",
    "    print('\\n')\n",
    "\n",
    "    # Define the hyperparameters to tune\n",
    "    param_grid = {\n",
    "        'objective': ['multi:softmax'],\n",
    "        'num_class': [3],  # Number of classes\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.1, 0.01, 0.001],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0],\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'random_state': [42]              #for reproducible results\n",
    "    }\n",
    "    print(f\"Adjusting hyperparameters to: {param_grid}\")\n",
    "    print('\\n')\n",
    "\n",
    "    # Use GridSearchCV to find the best hyperparameters\n",
    "    print(f\"Performing GridSearchCV\")\n",
    "    grid_search = GridSearchCV(clf, param_grid, cv=cv_count, scoring='accuracy')\n",
    "    print(f\"Fitting model\")\n",
    "    grid_search.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "    # Print the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    best_scores = grid_search.best_score_\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best Scores:\", best_scores)\n",
    "\n",
    "    # Evaluate the model with the best hyperparameters on the test set\n",
    "    clf = grid_search.best_estimator_\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # final cross validation\n",
    "    cross_val_score_result = cross_val_score(clf, X_train_resampled, y_train_label_resampled, cv=cv_count)\n",
    "    print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "    print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "    print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "    xgb_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "    xgb_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "\n",
    "    # Evaluate the model\n",
    "    Accuracy = accuracy_score(y_test_label, y_pred)\n",
    "    print(\"Accuracy:\", Accuracy)\n",
    "\n",
    "    # save best parameters for later comparison\n",
    "    best_params_xgb = best_params\n",
    "\n",
    "    # call previously defined function to create confusion matrix\n",
    "    cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "    # save results calculated for this model for later comparison to other models\n",
    "    accuracy_xgb_undersampled_optimized      = Accuracy\n",
    "    sensitivity_xgb_undersampled_optimized   = Sensitivity\n",
    "    specificity_xgb_undersampled_optimized   = Specificity\n",
    "    geometricmean_xgb_undersampled_optimized = GeometricMean\n",
    "    precision_xgb_undersampled_optimized     = Precision\n",
    "    recall_xgb_undersampled_optimized        = Recall\n",
    "    f1_xgb_undersampled_optimized            = F1\n",
    "\n",
    "    # show a running total of elapsed time for the entire notebook\n",
    "    show_elapsed_time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare acccuracy of LR, DT, DS, RF, NB, SVM, KNN, MLP, GB, XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this section compares the accuracy of different methods:\n",
    "\n",
    "if (is_data_scaled == \"yes\"): \n",
    "    print(f\"NOTE: This dataset has been scaled to avoid skewing the results due to large data distribution\")\n",
    "if (is_data_scaled == \"no\"): \n",
    "    print(f\"NOTE: This dataset has NOT been scaled, so the results may be inaccurate!\")\n",
    "print('\\n')\n",
    "          \n",
    "print(f\"LR  accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_lr_undersampled_unoptimized*100:.2f}%\")\n",
    "print(f\"LR  accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_lr_undersampled_optimized*100:.2f}%\")\n",
    "print('\\n')\n",
    "print(f\"DT  accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_dt_undersampled_unoptimized*100:.2f}%\")\n",
    "print(f\"DT  accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_dt_undersampled_optimized*100:.2f}%\")\n",
    "print('\\n')\n",
    "print(f\"DS  accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_ds_undersampled_unoptimized*100:.2f}%\")\n",
    "print(f\"DS  accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_ds_undersampled_optimized*100:.2f}%\")\n",
    "print('\\n')\n",
    "print(f\"RF  accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_rf_undersampled_unoptimized*100:.2f}%\")\n",
    "print(f\"RF  accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_rf_undersampled_optimized*100:.2f}%\")\n",
    "print('\\n')\n",
    "print(f\"NB  accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_nb_undersampled_unoptimized*100:.2f}%\")\n",
    "print(f\"NB  accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_nb_undersampled_optimized*100:.2f}%\")\n",
    "print('\\n')\n",
    "print(f\"SVM accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_svm_undersampled_unoptimized*100:.2f}%\")\n",
    "print(f\"SVM accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_svm_undersampled_optimized*100:.2f}%\")\n",
    "print('\\n')\n",
    "print(f\"KNN accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_knn_undersampled_unoptimized*100:.2f}%\")\n",
    "print(f\"KNN accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_knn_undersampled_optimized*100:.2f}%\")\n",
    "print('\\n')\n",
    "print(f\"MLP accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_mlp_undersampled_unoptimized*100:.2f}%\")\n",
    "print(f\"MLP accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_mlp_undersampled_optimized*100:.2f}%\")\n",
    "print('\\n')\n",
    "print(f\"GB  accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_gb_undersampled_unoptimized*100:.2f}%\")\n",
    "print(f\"GB  accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_gb_undersampled_optimized*100:.2f}%\")\n",
    "print('\\n')\n",
    "print(f\"XGB accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_xgb_undersampled_unoptimized*100:.2f}%\")\n",
    "print(f\"XGB accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_xgb_undersampled_optimized*100:.2f}%\")\n",
    "print('\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training with ensemble learning\n",
    "This section takes the individual ML algorithms tested earlier, then runs them through an ensemble model\n",
    "The goal is to see if ensemble learning can give us higher accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptions of ensemble classifiers\n",
    "\n",
    "__Voting Classifier:__  2 methods: hard voting (majority vote), and soft voting (takes the average of predictive probabilities, takes the class with the highest average probability)\n",
    "\n",
    "__Stacking Classifier:__ Generates a final model based on multiple base models.  Predictions in intermediate steps are used to generate meta-models.\n",
    "\n",
    "__Boosting Classifer:__ Trains weak model, generate new model on poorly performing instances, tweak the weights to get better accuracy.  The AdaBoostClassifier is an ensemble learning algorithm that belongs to the family of boosting methods. It is specifically designed for binary classification problems but can be extended to multi-class classification. AdaBoost stands for Adaptive Boosting, and its primary goal is to combine the predictions from multiple weak classifiers to create a strong classifier.\n",
    "\n",
    "__Bagging Classifier:__ Bagging (Bootstrap Aggregating) is an ensemble learning technique that aims to improve the stability and accuracy of machine learning models. It involves training multiple instances of the same base model on different subsets of the training data. The predictions from individual models are then combined, often by averaging or voting, to produce the final prediction.  BaggingClassifier is a powerful ensemble technique that is particularly effective when applied to base models with high variance. It offers improved generalization, stability, and robustness, but it may not be the optimal choice for all scenarios, and its effectiveness depends on the characteristics of the base model and the dataset.\n",
    "\n",
    "__Comparison Table__\n",
    "\n",
    "| Method   | Combines Models | Strengths                                                   | Weaknesses                              |\n",
    "|----------|-----------------|-------------------------------------------------------------|-----------------------------------------|\n",
    "| Voting   | Yes             | Simple, effective for balancing out model weaknesses.       | Not as sophisticated as other methods.  |\n",
    "| Stacking | Yes             | Can leverage the strengths of a combination of models.      | Risk of overfitting.                    |\n",
    "| Boosting | No              | Can turn a weak model into a strong one.                    | Sensitive to noisy data and outliers.   |\n",
    "| Bagging  | No              | Minimizes overfitting with data with high variance          | Depends on base model performance       |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup common parameters for all the EL models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test to be removed\n",
    "#best_params_svm = {'C': 10, 'kernel': 'rbf', 'probability': True, 'random_state': 42}\n",
    "#best_params_knn = {'n_neighbors': 10, 'weights': 'distance'}\n",
    "#best_params_mlp = {'alpha': 0.01, 'hidden_layer_sizes': (50, 25), 'max_iter': 300, 'random_state': 42}\n",
    "#best_params_gb = {}\n",
    "#best_params_xgb = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best parameters for LR:  {best_params_lr}\")\n",
    "print(f\"Best parameters for DT:  {best_params_dt}\")\n",
    "print(f\"Best parameters for DS:  {best_params_ds}\")\n",
    "print(f\"Best parameters for RF:  {best_params_rf}\")\n",
    "print(f\"Best parameters for NB:  {best_params_nb}\")\n",
    "print(f\"Best parameters for SVM: {best_params_svm}\")\n",
    "print(f\"Best parameters for KNN: {best_params_knn}\")\n",
    "print(f\"Best parameters for MLP: {best_params_mlp}\")\n",
    "print(f\"Best parameters for GB:  {best_params_gb}\")\n",
    "print(f\"Best parameters for XGB: {best_params_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the previous cell, we have optimized hyperparameters for each of the base classifiers saved in python dictionaries.\n",
    "# Now we will use the ** unpacking syntax to pass the key-value pairs from the dictionaries as keyword arguments to each classifier constructor.\n",
    "# This way, the hyperparameters specified in each dictionary are correctly applied when creating each individual classifier.\n",
    "\n",
    "# Define individual classifiers using hyperparameters calculated earlier\n",
    "lr_clf  = LogisticRegression(**best_params_lr)\n",
    "dt_clf  = DecisionTreeClassifier(**best_params_dt)\n",
    "ds_clf  = DecisionTreeClassifier(**best_params_ds)\n",
    "rf_clf  = RandomForestClassifier(**best_params_rf)\n",
    "nb_clf  = BernoulliNB(**best_params_nb)\n",
    "svm_clf = SVC(**best_params_svm)  #need probability=True for voting classifier, already set in hyperparameter optimization section\n",
    "knn_clf = KNeighborsClassifier(**best_params_knn)\n",
    "mlp_clf = MLPClassifier(**best_params_mlp)\n",
    "gb_clf  = GradientBoostingClassifier(**best_params_gb)\n",
    "xgb_clf = xgb.XGBClassifier(**best_params_xgb)\n",
    "\n",
    "print(f\"Best parameters for LR:  {lr_clf}\")\n",
    "print(f\"Best parameters for DT:  {dt_clf}\")\n",
    "print(f\"Best parameters for DS:  {ds_clf}\")\n",
    "print(f\"Best parameters for RF:  {rf_clf}\")\n",
    "print(f\"Best parameters for NB:  {nb_clf}\")\n",
    "print(f\"Best parameters for SVM: {svm_clf}\")\n",
    "print(f\"Best parameters for KNN: {knn_clf}\")\n",
    "print(f\"Best parameters for MLP: {mlp_clf}\")\n",
    "print(f\"Best parameters for GB:  {gb_clf}\")\n",
    "print(f\"Best parameters for XGB: {xgb_clf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting classifier\n",
    "\n",
    "In this example:\n",
    "\n",
    "SVC, KNeighborsClassifier, and RandomForestClassifier are individual classifiers.\n",
    "\n",
    "A VotingClassifier is created with these classifiers and a soft voting strategy. Soft voting predicts the class label based on the argmax of the sums of the predicted probabilities.\n",
    "\n",
    "The ensemble model is trained on the training set.\n",
    "\n",
    "Predictions are made on the test set, and the performance of the ensemble model is evaluated.\n",
    "\n",
    "You can adjust the parameters of the individual classifiers and the VotingClassifier based on your specific needs. Note that not all classifiers support probability estimates (probability=True), so make sure to check the documentation for each classifier.\n",
    "\n",
    "Ensemble methods like VotingClassifier are beneficial when combining diverse models that capture different aspects of the data, leading to a more robust and accurate overall model.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the voting classifier with all the base models\n",
    "\n",
    "# Create a VotingClassifier with 'soft' voting (uses predicted probabilities)\n",
    "clf = VotingClassifier(\n",
    "    estimators=[('lr', lr_clf), ('dt', dt_clf), ('rf', rf_clf), ('nb', nb_clf), ('svm', svm_clf), ('knn', knn_clf), ('mlp', mlp_clf), ('gb', gb_clf)],\n",
    "    voting='soft'  # 'hard' for majority voting, 'soft' for weighted voting based on probabilities\n",
    ")\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "Accuracy = clf.score(X_test, y_test_label)\n",
    "print(\"Accuracy:\", Accuracy)\n",
    "\n",
    "# save accuracy for later comparison\n",
    "accuracy_ensemble_voting = Accuracy\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the voting classifier with stronger learners to see if you get better accuracy\n",
    "\n",
    "# Create a VotingClassifier with 'soft' voting (uses predicted probabilities)\n",
    "clf = VotingClassifier(\n",
    "    estimators=[('svm', svm_clf), ('rf', rf_clf), ('dt', dt_clf)],\n",
    "    voting='soft'  # 'hard' for majority voting, 'soft' for weighted voting based on probabilities\n",
    ")\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "Accuracy = clf.score(X_test, y_test_label)\n",
    "print(\"Accuracy:\", Accuracy)\n",
    "\n",
    "# save accuracy for later comparison\n",
    "accuracy_ensemble_voting = Accuracy\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the voting classifier with the weakest base models\n",
    "\n",
    "# Create a VotingClassifier with 'soft' voting (uses predicted probabilities)\n",
    "clf = VotingClassifier(\n",
    "    estimators=[('lr', lr_clf), ('nb', nb_clf), ('svm', svm_clf), ('knn', knn_clf), ('mlp', mlp_clf)],\n",
    "    voting='soft'  # 'hard' for majority voting, 'soft' for weighted voting based on probabilities\n",
    ")\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# final cross validation\n",
    "cross_val_score_result = cross_val_score(clf, X_train_resampled, y_train_label_resampled, cv=cv_count)\n",
    "print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "voting_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "voting_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "# Evaluate the model\n",
    "Accuracy = accuracy_score(y_test_label, y_pred)\n",
    "print(\"Accuracy:\", Accuracy)\n",
    "\n",
    "# save best parameters for later comparison\n",
    "best_params_ensemble_voting = clf\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "accuracy_ensemble_voting      = Accuracy\n",
    "sensitivity_ensemble_voting   = Sensitivity\n",
    "specificity_ensemble_voting   = Specificity\n",
    "geometricmean_ensemble_voting = GeometricMean\n",
    "precision_ensemble_voting     = Precision\n",
    "recall_ensemble_voting        = Recall\n",
    "f1_ensemble_voting            = F1\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following cell takes ~20 minutes to run, but tuning the hyperparameters, does not improve the accuracy over the previous cell,\n",
    "# so the following cell has been commented out to save processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the VotingClassifier\n",
    "# clf = VotingClassifier(estimators=[('lr', lr_clf), ('svm', svm_clf), ('nb', nb_clf), ('knn', knn_clf), ('mlp', mlp_clf)])\n",
    "\n",
    "\n",
    "# # Define the hyperparameters to tune\n",
    "# param_grid = {\n",
    "#     'voting': ['hard', 'soft'],                    # Include options for hard and soft voting\n",
    "#     'weights': [None, [1, 2, 1, 2, 3]]             # If using soft voting, you can also optimize the weights, higher values mean more weight for that estimator\n",
    "#     #'lr__C':  [0.1, 1, 10],                       # LR  hyperparameter tuning not required because parameters have already been optimized, so tuning here would be redundant\n",
    "#     #'svm__C': [0.1, 1, 10],                       # SVC hyperparameter tuning not required because parameters have already been optimized, so tuning here would be redundant\n",
    "#     #'knn__n_neighbors': [5, 10, 30],              # KNN hyperparameter tuning not required because parameters have already been optimized, so tuning here would be redundant\n",
    "#     #'nb__alpha':  [0.1, 0.01, 0.001],             # NB  hyperparameter tuning not required because parameters have already been optimized, so tuning here would be redundant\n",
    "#     #'mlp__alpha': [0.1, 0.01, 0.001]              # MLP hyperparameter tuning not required because parameters have already been optimized, so tuning here would be redundant\n",
    "# }\n",
    "\n",
    "\n",
    "# # Use GridSearchCV for hyperparameter tuning\n",
    "# print(f\"Performing GridSearchCV\")\n",
    "# grid_search = GridSearchCV(clf, param_grid, cv=cv_count, scoring='accuracy')\n",
    "# print(f\"Fitting model\")\n",
    "# grid_search.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# # Validate on Test Set\n",
    "# clf = grid_search.best_estimator_\n",
    "# print(f\"Found best_estimator_  {clf}\")\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "# # final cross validation\n",
    "# cross_val_score_result = cross_val_score(clf, X_train_resampled, y_train_label_resampled, cv=cv_count)\n",
    "# print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "# print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "# print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "# voting_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "# voting_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "# # Evaluate the model\n",
    "# Accuracy = accuracy_score(y_test_label, y_pred)\n",
    "# print(\"Accuracy:\", Accuracy)\n",
    "\n",
    "# # save best parameters for later comparison\n",
    "# best_params_ensemble_voting = clf\n",
    "\n",
    "# # call previously defined function to create confusion matrix\n",
    "# cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "# # save results calculated for this model for later comparison to other models\n",
    "# accuracy_ensemble_voting      = Accuracy\n",
    "# sensitivity_ensemble_voting   = Sensitivity\n",
    "# specificity_ensemble_voting   = Specificity\n",
    "# geometricmean_ensemble_voting = GeometricMean\n",
    "# precision_ensemble_voting     = Precision\n",
    "# recall_ensemble_voting        = Recall\n",
    "# f1_ensemble_voting            = F1\n",
    "\n",
    "# # show a running total of elapsed time for the entire notebook\n",
    "# show_elapsed_time() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the VotingClassifier\n",
    "clf = VotingClassifier(estimators=[('lr', lr_clf), ('svm', svm_clf), ('nb', nb_clf), ('knn', knn_clf), ('mlp', mlp_clf)])\n",
    "\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'voting': ['hard', 'soft'],                    # Include options for hard and soft voting\n",
    "    'weights': [None, [1, 2, 1, 2, 3]]             # If using soft voting, you can also optimize the weights, higher values mean more weight for that estimator\n",
    "     #'lr__C':  [0.1, 1, 10],                      # LR  hyperparameter tuning not required because parameters have already been optimized, so tuning here would be redundant\n",
    "     #'svm__C': [0.1, 1, 10],                      # SVC hyperparameter tuning not required because parameters have already been optimized, so tuning here would be redundant\n",
    "     #'knn__n_neighbors': [5, 10, 30],             # KNN hyperparameter tuning not required because parameters have already been optimized, so tuning here would be redundant\n",
    "     #'nb__alpha':  [0.1, 0.01, 0.001],            # NB  hyperparameter tuning not required because parameters have already been optimized, so tuning here would be redundant\n",
    "     #'mlp__alpha': [0.1, 0.01, 0.001]             # MLP hyperparameter tuning not required because parameters have already been optimized, so tuning here would be redundant\n",
    "}\n",
    "\n",
    "\n",
    "# Use GridSearchCV for hyperparameter tuning\n",
    "print(f\"Performing GridSearchCV\")\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=cv_count, scoring='accuracy')\n",
    "print(f\"Fitting model\")\n",
    "grid_search.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Validate on Test Set\n",
    "clf = grid_search.best_estimator_\n",
    "print(f\"Found best_estimator_  {clf}\")\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# final cross validation\n",
    "cross_val_score_result = cross_val_score(clf, X_train_resampled, y_train_label_resampled, cv=cv_count)\n",
    "print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "voting_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "voting_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "# Evaluate the model\n",
    "Accuracy = accuracy_score(y_test_label, y_pred)\n",
    "print(\"Accuracy:\", Accuracy)\n",
    "\n",
    "# save best parameters for later comparison\n",
    "best_params_ensemble_voting = clf\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "## save results calculated for this model for later comparison to other models\n",
    "#accuracy_ensemble_voting      = Accuracy\n",
    "#sensitivity_ensemble_voting   = Sensitivity\n",
    "#specificity_ensemble_voting   = Specificity\n",
    "#geometricmean_ensemble_voting = GeometricMean\n",
    "#precision_ensemble_voting     = Precision\n",
    "#recall_ensemble_voting        = Recall\n",
    "#f1_ensemble_voting            = F1\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking classifier\n",
    "\n",
    "This model (StackingClassifier) uses multiple base estimators such as LR, NB, SVC, KNN, etc.\n",
    "\n",
    "A StackingClassifier is created with these multiple base classifiers classifiers and a meta-classifier (LogisticRegression) as the final estimator.\n",
    "\n",
    "The stacking ensemble model is trained on the training set.\n",
    "\n",
    "Predictions are made on the test set, and the performance of the stacking ensemble model is evaluated.\n",
    "\n",
    "You can customize the base estimators, the final estimator, and other parameters of the StackingClassifier based on your specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try all the base estimators with the default final_estimator\n",
    "\n",
    "# Create a stacking ensemble model with a logistic regression meta-classifier\n",
    "clf = StackingClassifier(\n",
    "    estimators=[('lr', lr_clf), ('dt', dt_clf), ('rf', rf_clf), ('nb', nb_clf), ('svm', svm_clf), ('knn', knn_clf), ('mlp', mlp_clf), ('gb', gb_clf)],\n",
    "    final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "Accuracy = clf.score(X_test, y_test_label)\n",
    "print(\"Accuracy:\", Accuracy)\n",
    "\n",
    "# save accuracy for later comparison\n",
    "accuracy_ensemble_stacking = Accuracy\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try only the strongest base classifiers in the stacking classifier,  with the default final_estimator\n",
    "\n",
    "clf = StackingClassifier(\n",
    "    estimators=[('dt', dt_clf), ('rf', rf_clf),  ('gb', gb_clf)],\n",
    "    final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "Accuracy = clf.score(X_test, y_test_label)\n",
    "print(\"Accuracy:\", Accuracy)\n",
    "\n",
    "# save accuracy for later comparison\n",
    "accuracy_ensemble_stacking = Accuracy\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try only the weakest base models with the default final_estimator\n",
    "\n",
    "# Create a stacking ensemble model with a logistic regression meta-classifier\n",
    "clf = StackingClassifier(\n",
    "    estimators=[('lr', lr_clf), ('nb', nb_clf), ('svm', svm_clf), ('knn', knn_clf), ('mlp', mlp_clf)],\n",
    "    final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# final cross validation\n",
    "cross_val_score_result = cross_val_score(clf, X_train_resampled, y_train_label_resampled, cv=cv_count)\n",
    "print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "stacking_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "stacking_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "# Evaluate the model\n",
    "Accuracy = clf.score(X_test, y_test_label)\n",
    "print(\"Accuracy:\", Accuracy)\n",
    "\n",
    "# save best parameters for later comparison\n",
    "best_params_ensemble_stacking = clf\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "accuracy_ensemble_stacking      = Accuracy\n",
    "sensitivity_ensemble_stacking   = Sensitivity\n",
    "specificity_ensemble_stacking   = Specificity\n",
    "geometricmean_ensemble_stacking = GeometricMean\n",
    "precision_ensemble_stacking     = Precision\n",
    "recall_ensemble_stacking        = Recall\n",
    "f1_ensemble_stacking            = F1\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call previously defined function to create confusion matrix\n",
    "cm = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following cell takes ~8 hours to run all the final_estimators, but does increase the accuracy by ~4%\n",
    "\n",
    "# After running the following cell once, we know that a final_estimator of SVM gives the highest accuracy\n",
    "\n",
    "# Each final_estimator takes approximately this much time to run:\n",
    "#   NB   40 minutes, accuracy 0.5140\n",
    "#   SVM  55 minutes, accuracy 0.9134\n",
    "#   KNN  45 minutes, accuracy 0.9008\n",
    "#   MLP 180 minutes, accuracy 0.9119\n",
    "#   LR  140 minutes, accuracy 0.8717\n",
    "# So let's speed up subsequent executions by only skipping everything except SVM as the final_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with multiple weak base_estimators, then create a for loop to test each final_estimator, keeping track of the best final_estimator\n",
    "\n",
    "\n",
    "estimator_type = \"weak\"   #strong|weak flag to determine which base estimators to use\n",
    "\n",
    "strong_base_estimators = [('rf', rf_clf), ('gb', gb_clf), ('dt', dt_clf)]\n",
    "weak_base_estimators   = [('lr', lr_clf), ('nb', nb_clf), ('svm', svm_clf), ('knn', knn_clf), ('mlp', mlp_clf)]\n",
    "#final_estimators       = ['RandomForestClassifier', 'DecisionTreeClassifier', 'GradientBoostingClassifier','LogisticRegression','BernoulliNB', 'SVC', 'KNN', 'MLPClassifier']\n",
    "#final_estimators       = ['BernoulliNB', 'SVC', 'KNN', 'MLPClassifier', 'LogisticRegression']\n",
    "#final_estimators       = ['BernoulliNB', 'SVC', 'KNN', 'MLPClassifier', 'LogisticRegression']\n",
    "final_estimators       = ['SVC']  #we know SVC gives the highest accuracy, save time by skipping the other final_estimators\n",
    "\n",
    "\n",
    "if (estimator_type == \"strong\"): base_estimators = strong_base_estimators\n",
    "if (estimator_type == \"weak\"):   base_estimators = weak_base_estimators\n",
    "\n",
    "best_final_estimator_name     = \"none\"\n",
    "best_final_estimator_accuracy = 0   #initialize value to keep track of the accuracy level of each final classifier\n",
    "\n",
    "for my_final_estimator in final_estimators:\n",
    "    print('\\n')\n",
    "    print(f\"Testing hyperparameter optimization with {estimator_type} base estimators {base_estimators} and final_estimator={my_final_estimator}\")\n",
    "    \n",
    "    if (my_final_estimator == 'RandomForestClassifier'):  \n",
    "        ensemble = StackingClassifier(estimators=base_estimators, final_estimator=RandomForestClassifier())\n",
    "        ensemble_params = {'final_estimator__n_estimators': [50, 100, 200], 'final_estimator__max_depth': [None, 5, 10, 15]}  #tunable hyperparameters for final_estimator\n",
    "    if (my_final_estimator == 'DecisionTreeClassifier'): \n",
    "        ensemble = StackingClassifier(estimators=base_estimators, final_estimator=DecisionTreeClassifier())\n",
    "        ensemble_params = {'final_estimator__max_depth': [None, 5, 10, 15]}  #tunable hyperparameters for final_estimator\n",
    "    if (my_final_estimator == 'GradientBoostingClassifier'): \n",
    "        ensemble = StackingClassifier(estimators=base_estimators, final_estimator=GradientBoostingClassifier())\n",
    "        ensemble_params = {'final_estimator__n_estimators': [10, 100, 300], 'final_estimator__learning_rate': [0.1, 0.01, 0.2], 'final_estimator__max_depth': [3,5,10]}  #tunable hyperparameters for final_estimator\n",
    "    if (my_final_estimator == 'LogisticRegression'): \n",
    "        ensemble = StackingClassifier(estimators=base_estimators, final_estimator=LogisticRegression())\n",
    "        ensemble_params = {'final_estimator__C': [1, 10, 100], 'final_estimator__max_iter': [100, 200, 300]}  #tunable hyperparameters for final_estimator\n",
    "    if (my_final_estimator == 'BernoulliNB'): \n",
    "        ensemble = StackingClassifier(estimators=base_estimators, final_estimator=BernoulliNB())\n",
    "        ensemble_params = {'final_estimator__alpha': [0.1, 0.001]}  #tunable hyperparameters for final_estimator\n",
    "    if (my_final_estimator == 'SVC'): \n",
    "        ensemble = StackingClassifier(estimators=base_estimators, final_estimator=SVC())\n",
    "        ensemble_params = {'final_estimator__C': [1, 10]}  #tunable hyperparameters for final_estimator\n",
    "    if (my_final_estimator == 'KNN'): \n",
    "        ensemble = StackingClassifier(estimators=base_estimators, final_estimator=KNeighborsClassifier())\n",
    "        ensemble_params = {'final_estimator__n_neighbors': [10,30]}  #tunable hyperparameters for final_estimator\n",
    "    if (my_final_estimator == 'MLPClassifier'): \n",
    "        ensemble = StackingClassifier(estimators=base_estimators, final_estimator=MLPClassifier())\n",
    "        ensemble_params = {'final_estimator__hidden_layer_sizes': [(100, 50), (50, 25), (150, 100)], 'final_estimator__max_iter': [500, 800], 'final_estimator__alpha': [0.001, 0.01]}  #tunable hyperparameters for final_estimator\n",
    "        \n",
    "    print(f\"Performing GridSearchCV for final_estimator={my_final_estimator}\")\n",
    "    ensemble_grid = GridSearchCV(ensemble, ensemble_params, cv=cv_count, scoring='accuracy')\n",
    "    print(f\"Fitting model\")\n",
    "    ensemble_grid.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "    # Validate on Test Set\n",
    "    clf = ensemble_grid.best_estimator_\n",
    "    print(f\"Found best_estimator_  {clf}\")\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # final cross validation\n",
    "    cross_val_score_result = cross_val_score(clf, X_train_resampled, y_train_label_resampled, cv=cv_count)\n",
    "    print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "    print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "    print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "    stacking_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "    stacking_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "\n",
    "    # Evaluate the model\n",
    "    Accuracy = accuracy_score(y_test_label, y_pred)\n",
    "    print(\"Accuracy:\", Accuracy)\n",
    "    \n",
    "    # of all the final_estimators, check to see if this final_estimator provides the best accuracy\n",
    "    if (Accuracy > best_final_estimator_accuracy):\n",
    "        best_final_estimator_name     = my_final_estimator  #save the name of the final_estimator that is currently the best\n",
    "        best_final_estimator_accuracy = Accuracy            #save the accuracy of the final estimator that is currently the best\n",
    "        print(f\"The best final_estimator so far is {best_final_estimator_name}, with accuracy of {best_final_estimator_accuracy}\")\n",
    "    else:\n",
    "        print(f\"This is not the best base classifier\")\n",
    "\n",
    "    # save best parameters for later comparison\n",
    "    best_params_ensemble_stacking = clf\n",
    "\n",
    "    # call previously defined function to create confusion matrix\n",
    "    cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "    # save results calculated for this model for later comparison to other models\n",
    "    accuracy_ensemble_stacking      = Accuracy\n",
    "    sensitivity_ensemble_stacking   = Sensitivity\n",
    "    specificity_ensemble_stacking   = Specificity\n",
    "    geometricmean_ensemble_stacking = GeometricMean\n",
    "    precision_ensemble_stacking     = Precision\n",
    "    recall_ensemble_stacking        = Recall\n",
    "    f1_ensemble_stacking            = F1\n",
    "\n",
    "    # show a running total of elapsed time for the entire notebook\n",
    "    show_elapsed_time() \n",
    "\n",
    "\n",
    "# after testing all the final_estimators, display the best one\n",
    "print(f\"After checking each final_estimator, the best final_estimator is {best_final_estimator_name}, with accuracy of {best_final_estimator_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following cell takes ~200 minutes to run \n",
    "\n",
    "# The following cell takes ~8 hours to run all the final_estimators, but does increase the accuracy by ~4%\n",
    "\n",
    "# After running the following cell once, we know that a final_estimator of MLP gives the highest accuracy\n",
    "\n",
    "# Each final_estimator takes approximately this much time to run:\n",
    "#   NB    2 minutes, accuracy 0.7700\n",
    "#   SVM 180 minutes\n",
    "#   KNN  10 minutes\n",
    "#   MLP  10 minutes, accuracy 0.8946\n",
    "#   LR    2 minutes, accuracy 0.8756\n",
    "# So let's speed up subsequent executions by  skipping the time-consuming SVM and KNN since we know they are not the best base_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging can only use a single base classifier\n",
    "# Use a for loop to test all the base classifiers with bagging, one base classifier at a time, keeping track of the best_final_estimator_name\n",
    "\n",
    "\n",
    "best_base_classifier_name     = \"none\"\n",
    "best_base_classifier_accuracy = 0   #initialize value to keep track of the accuracy level of each base classifier\n",
    "\n",
    "base_classifiers = [lr_clf, dt_clf, rf_clf, nb_clf, svm_clf, knn_clf, mlp_clf, gb_clf, xgb_clf]  #xgb_clf causing error?\n",
    "base_classifiers = [lr_clf, dt_clf, rf_clf, nb_clf, svm_clf, knn_clf, mlp_clf, gb_clf]           # all classifiers\n",
    "base_classifiers = [dt_clf, rf_clf, gb_clf]                                                      # strong learners\n",
    "base_classifiers = [lr_clf, nb_clf, svm_clf, knn_clf, mlp_clf]                                   # weak learners\n",
    "base_classifiers = [lr_clf, nb_clf, mlp_clf]                                                     # remove SVM and KNN because we know from testing they are not the optimal base_classifier\n",
    "for base_classifier in base_classifiers:\n",
    "    print(\"\\n\")\n",
    "    print(f\"------------------------------------\")\n",
    "    print(f\"Base classifier is {base_classifier}\")\n",
    "    print(f\"------------------------------------\")\n",
    "\n",
    "    # Define the BaggingClassifier\n",
    "    clf = BaggingClassifier(base_classifier, n_estimators=50, random_state=42)\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    clf.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Final cross validation\n",
    "    cross_val_score_result = cross_val_score(clf, X_train_resampled, y_train_label_resampled, cv=cv_count)\n",
    "    print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "    print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "    print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "    bagging_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "    bagging_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "\n",
    "    # Evaluate the model\n",
    "    Accuracy = accuracy_score(y_test_label, y_pred)\n",
    "    print(f\"Accuracy on Test Set: {Accuracy}\")\n",
    "\n",
    "\n",
    "    # of all the base_classifiers, check to see if this base_classifier provides the best accuracy\n",
    "    if (accuracy > best_base_classifier_accuracy):\n",
    "        best_base_classifier_name     = base_classifier     #save the name of the base_classifier that is currently the best\n",
    "        best_base_classifier_accuracy = accuracy            #save the accuracy of the final estimator that is currently the best\n",
    "        print(f\"The best base_classifier so far is {best_base_classifier_name}, with accuracy of {best_base_classifier_accuracy}\")\n",
    "    else:\n",
    "        print(f\"This is not the best base classifier\")\n",
    " \n",
    "\n",
    "    # call previously defined function to create confusion matrix\n",
    "    cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "    # save results calculated for this model for later comparison to other models\n",
    "    accuracy_ensemble_bagging      = Accuracy\n",
    "    sensitivity_ensemble_bagging   = Sensitivity\n",
    "    specificity_ensemble_bagging   = Specificity\n",
    "    geometricmean_ensemble_bagging = GeometricMean\n",
    "    precision_ensemble_bagging     = Precision\n",
    "    recall_ensemble_bagging        = Recall\n",
    "    f1_ensemble_bagging            = F1\n",
    "\n",
    "    # show a running total of elapsed time for the entire notebook\n",
    "    show_elapsed_time() \n",
    "\n",
    "\n",
    "# after testing all the final_estimators, display the best one\n",
    "print(f\"After checking each base_classifier, the best base_classifier is {best_base_classifier_name}, with accuracy of {best_base_classifier_accuracy}, and best_params of {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HINT: in sklearn.ensemble.BaggingClassifier version 1.2.0, the \"base_estimator\" parameter was renamed to \"estimator\"\n",
    "# The base_estimator parameter is deprecated in sklearn version 1.2.0, and will be removed in version 1.4.0\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\n",
    "# Check to see if this version of BaggingClassifer() expects to have a \"base_estimator\" or \"estimator\" parameter\n",
    "\n",
    "\n",
    "# Print the version of scikit-learn\n",
    "print(\"Currently installed scikit-learn version is:\", sklearn.__version__)\n",
    "\n",
    "# Create an instance of the BaggingClassifier model\n",
    "clf = BaggingClassifier()\n",
    "\n",
    "# Figure out which parameters exist\n",
    "default_params = clf.get_params()\n",
    "print(f\"Default parameters are {default_params}\")\n",
    "\n",
    "# Check to see if the base_estimator parameter exists in the BaggingClassifier, which would indicate an outdated version of scikit-learn\n",
    "desired_parameter1 = 'base_estimator'  # Replace with the parameter you want to check\n",
    "desired_parameter2 = 'estimator'  # Replace with the parameter you want to check\n",
    "\n",
    "# This if block will only be executed if the scikit-learn package is older than 1.2\n",
    "if (desired_parameter1 in clf.get_params()) and not (desired_parameter2 in clf.get_params()) :\n",
    "    print('\\n')\n",
    "    print(f\"WARNING: the '{desired_parameter1}' parameter exists, but the '{desired_parameter2}' parameter does not exist the BaggingClassifier.\")\n",
    "    print(f\"The parameter 'base_estimator' was deprecated in favor of 'estimator' in sklearn 1.2.0, will be removed entirely in sklearn 1.4.0.\")\n",
    "    print(f\"Your currently installed version of scikit-learn is\", sklearn.__version__)\n",
    "    print(f\"You may wish to update your installed version of scikit-learn to a minimum of 1.2.0 so you can use the 'estimator__' parameter in the next cell.\")\n",
    "    print(f\"If you are unable to update your installed version of scikit-learn, you will need to change 'estimator__' to 'base_estimator__' in the following cell for compatibility with your version of scikit-learn.\")\n",
    "    print(f\"If you are     using Anaconda Navigator, you can upgrade with:  conda update conda, conda update scikit-learn\")\n",
    "    print(f\"If you are not using Anaconda Navigator, you can upgrade with:  pip install --upgrade scikit-learn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following cell takes ~10 hours to run, but tuning the hyperparameters, does not improve the accuracy over the previous cell,\n",
    "# so the following cell has been commented out to save processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Try different weak learners with different BaggingClassifier parameters, keeping track of which base_estimator provides the best accuracy\n",
    "\n",
    "# best_base_estimator_name     = \"none\"\n",
    "# best_base_estimator_accuracy = 0   #initialize value to keep track of the accuracy level of each base classifier\n",
    "\n",
    "\n",
    "# base_estimators = ['lr', 'nb', 'svm', 'mlp', 'knn']                                  # weak learners\n",
    "# for base_estimator in base_estimators:\n",
    "#     print(\"\\n\")\n",
    "#     print(f\"------------------------------------\")\n",
    "#     print(f\"Base estimator is {base_estimator}\")\n",
    "#     print(f\"------------------------------------\")\n",
    "\n",
    "#     if (base_estimator == 'lr'):\n",
    "#         clf = BaggingClassifier(LogisticRegression(), random_state=42)            # Define the BaggingClassifier \n",
    "#         param_grid = {'estimator__penalty': [best_params_lr['penalty']],          # optimized hyperparameter from base_estimator\n",
    "#                       'estimator__C': [best_params_lr['C']],                      # optimized hyperparameter from base_estimator\n",
    "#                       'estimator__solver': [best_params_lr['solver']],            # optimized hyperparameter from base_estimator\n",
    "#                       'estimator__max_iter': [best_params_lr['max_iter']],        # optimized hyperparameter from base_estimator\n",
    "#                       'n_estimators': [100],                                      # Number of base estimators\n",
    "#                       'max_samples': [1.0],                                       # The proportion of samples  to draw from X to train each base estimator\n",
    "#                       'max_features': [1.0]                                       # The proportion of features to draw from X to train each base estimator\n",
    "#                      }\n",
    "\n",
    "#     if (base_estimator == 'nb'):\n",
    "#         clf = BaggingClassifier(BernoulliNB(), random_state=42)                   # Define the BaggingClassifier\n",
    "#         param_grid = {'estimator__alpha': [best_params_nb['alpha']],              # optimized hyperparameter from base_estimator\n",
    "#                       'n_estimators': [50, 100, 200],                             # Number of base estimators\n",
    "#                       'max_samples': [0.5, 0.7, 1.0],                             # The proportion of samples  to draw from X to train each base estimator\n",
    "#                       'max_features': [0.5, 0.7, 1.0]                             # The proportion of features to draw from X to train each base estimator\n",
    "#                      }\n",
    "\n",
    "\n",
    "#     if (base_estimator == 'svm'):\n",
    "#         clf = BaggingClassifier(SVC(), random_state=42)                           # Define the BaggingClassifier\n",
    "#         param_grid = {'estimator__C': [best_params_svm['C']],                     # optimized hyperparameter from base_estimator\n",
    "#                       'estimator__kernel': [best_params_svm['kernel']],           # optimized hyperparameter from base_estimator\n",
    "#                       'n_estimators': [200],                                      # Number of base estimators\n",
    "#                       'max_samples': [1.0],                                       # The proportion of samples  to draw from X to train each base estimator\n",
    "#                       'max_features': [1.0]                                       # The proportion of features to draw from X to train each base estimator\n",
    "#                      }\n",
    "\n",
    "#     if (base_estimator == 'knn'):\n",
    "#         clf = BaggingClassifier(KNeighborsClassifier(), random_state=42)           # Define the BaggingClassifier\n",
    "#         param_grid = {'estimator__n_neighbors': [best_params_knn['n_neighbors']],  # optimized hyperparameter from base_estimator\n",
    "#                       'estimator__weights': [best_params_knn['weights']],          # optimized hyperparameter from base_estimator\n",
    "#                       'n_estimators': [100],                                       # Number of base estimators\n",
    "#                       'max_samples': [1.0],                                        # The proportion of samples  to draw from X to train each base estimator\n",
    "#                       'max_features': [0.5]                                        # The proportion of features to draw from X to train each base estimator\n",
    "#                      }\n",
    "\n",
    "#     if (base_estimator == 'mlp'):\n",
    "#         clf = BaggingClassifier(MLPClassifier(), random_state=42)                   # Define the BaggingClassifier\n",
    "#         param_grid = {'estimator__hidden_layer_sizes': [best_params_mlp['hidden_layer_sizes']],   # optimized hyperparameter from base_estimator\n",
    "#                       'estimator__max_iter': [best_params_mlp['max_iter']],         # optimized hyperparameter from base_estimator\n",
    "#                       'estimator__alpha': [best_params_mlp['alpha']],               # optimized hyperparameter from base_estimator\n",
    "#                       'n_estimators': [100],                                        # Number of base estimators\n",
    "#                       'max_samples': [1.0],                                         # The proportion of samples  to draw from X to train each base estimator\n",
    "#                       'max_features': [0.5]                                         # The proportion of features to draw from X to train each base estimator\n",
    "#                      }\n",
    "\n",
    "        \n",
    "#     # Use GridSearchCV for hyperparameter tuning\n",
    "#     print(f\"Performing GridSearchCV\")\n",
    "#     grid_search = GridSearchCV(clf, param_grid, cv=cv_count, scoring='accuracy')\n",
    "#     print(f\"Fitting model\")\n",
    "#     grid_search.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "    \n",
    "#     # Print the best hyperparameters\n",
    "#     best_params = grid_search.best_params_\n",
    "#     best_scores = grid_search.best_score_\n",
    "#     print(\"Best Parameters:\", best_params)\n",
    "#     print(\"Best Scores:\", best_scores)\n",
    "\n",
    "#     # Evaluate the model with the best hyperparameters on the test set\n",
    "#     clf = grid_search.best_estimator_\n",
    "#     y_pred = clf.predict(X_test)\n",
    "\n",
    "#     # Final cross validation\n",
    "#     cross_val_score_result = cross_val_score(clf, X_train_resampled, y_train_label_resampled, cv=cv_count)\n",
    "#     print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "#     print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "#     print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "#     bagging_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "#     bagging_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "\n",
    "#     # Evaluate the model\n",
    "#     Accuracy = accuracy_score(y_test_label, y_pred)\n",
    "#     print(f\"Accuracy on Test Set: {Accuracy}\")\n",
    "\n",
    "#     # of all the base_classifiers, check to see if this base_classifier provides the best accuracy\n",
    "#     if (Accuracy > best_base_estimator_accuracy):\n",
    "#         best_params_ensemble_bagging = best_params         #save best parameters for later comparison\n",
    "#         best_base_estimator_name     = base_estimator      #save the name of the base_classifier that is currently the best\n",
    "#         best_base_estimator_accuracy = accuracy            #save the accuracy of the final estimator that is currently the best\n",
    "#         print(f\"The best base_estimator so far is {best_base_estimator_name}, with accuracy of {best_base_estimator_accuracy}\")\n",
    "#     else:\n",
    "#         print(f\"This is not the best base estimator\")\n",
    "        \n",
    "#     # call previously defined function to create confusion matrix\n",
    "#     cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "#     # save results calculated for this model for later comparison to other models\n",
    "#     accuracy_ensemble_bagging      = Accuracy\n",
    "#     sensitivity_ensemble_bagging   = Sensitivity\n",
    "#     specificity_ensemble_bagging   = Specificity\n",
    "#     geometricmean_ensemble_bagging = GeometricMean\n",
    "#     precision_ensemble_bagging     = Precision\n",
    "#     recall_ensemble_bagging        = Recall\n",
    "#     f1_ensemble_bagging            = F1\n",
    "\n",
    "#     # show a running total of elapsed time for the entire notebook\n",
    "#     show_elapsed_time() \n",
    "\n",
    "\n",
    "# # after testing all the final_estimators, display the best one\n",
    "# print(f\"After checking each base_estimator, the best base_estimator is {best_base_estimator_name}, with accuracy of {best_base_estimator_accuracy}, and best_params of {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Classifier\n",
    "\n",
    "In this example:\n",
    "\n",
    "LR, SVC, KNN, NB, MLP are individual base classifiers.\n",
    "\n",
    "An AdaBoostClassifier is created with these base classifiers.\n",
    "\n",
    "The AdaBoost classifier is trained on the training set.\n",
    "\n",
    "Predictions are made on the test set, and the performance of the AdaBoost classifier is evaluated.\n",
    "\n",
    "You can adjust the parameters such as n_estimators and learning_rate based on your specific needs. Note that AdaBoost works best with weak learners, so base classifiers like decision trees with limited depth are commonly used.\n",
    "\n",
    "The AdaBoostClassifier can use different base classifiers (weak learners) as its base estimator. The base_estimator parameter of the AdaBoostClassifier allows you to specify the type of weak learner to use. If not specified, the default is a decision stump (DecisionTreeClassifier(max_depth=1)).\n",
    "\n",
    "Using RandomForestClassifier as a base estimator for AdaBoostClassifier is generally not a common practice because AdaBoost is typically used with weak learners, and Random Forests are already ensemble methods that use multiple decision trees.\n",
    "\n",
    "However, if you still want to experiment with this combination, you can specify RandomForestClassifier as a base_estimator in AdaBoostClassifier.\n",
    "\n",
    "Keep in mind that using RandomForestClassifier as a base estimator for AdaBoost might not provide significant advantages, as Random Forests are already powerful ensemble models. AdaBoost is often more beneficial when combined with weak learners like shallow decision trees (stumps). It's recommended to experiment with different combinations and evaluate their performance on your specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HINT: in sklearn.ensemble.AdaBoostClassifier version 1.2.0, the \"base_estimator\" parameter was renamed to \"estimator\"\n",
    "# The base_estimator parameter is deprecated in sklearn version 1.2.0, and will be removed in version 1.4.0\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\n",
    "# Check to see if this version of AdaBoostClassifer() expects to have a \"base_estimator\" or \"estimator\" parameter\n",
    "\n",
    "\n",
    "# Print the version of scikit-learn\n",
    "print(\"Currently installed scikit-learn version is:\", sklearn.__version__)\n",
    "\n",
    "# Create an instance of the BaggingClassifier model\n",
    "clf = AdaBoostClassifier()\n",
    "\n",
    "# Figure out which parameters exist\n",
    "default_params = clf.get_params()\n",
    "print(f\"Default parameters are {default_params}\")\n",
    "\n",
    "# Check to see if the base_estimator parameter exists in the BaggingClassifier, which would indicate an outdated version of scikit-learn\n",
    "desired_parameter1 = 'base_estimator'  # Replace with the parameter you want to check\n",
    "desired_parameter2 = 'estimator'  # Replace with the parameter you want to check\n",
    "\n",
    "# This if block will only be executed if the scikit-learn package is older than 1.2\n",
    "if (desired_parameter1 in clf.get_params()) and not (desired_parameter2 in clf.get_params()) :\n",
    "    print('\\n')\n",
    "    print(f\"WARNING: the '{desired_parameter1}' parameter exists, but the '{desired_parameter2}' parameter does not exist the AdaBoostClassifier.\")\n",
    "    print(f\"The parameter 'base_estimator' was deprecated in favor of 'estimator' in sklearn 1.2.0, will be removed entirely in sklearn 1.4.0.\")\n",
    "    print(f\"Your currently installed version of scikit-learn is\", sklearn.__version__)\n",
    "    print(f\"You may wish to update your installed version of scikit-learn to a minimum of 1.2.0 so you can use the 'estimator__' parameter in the next cell.\")\n",
    "    print(f\"If you are unable to update your installed version of scikit-learn, you will need to change 'estimator__' to 'base_estimator__' in the following cell for compatibility with your version of scikit-learn.\")\n",
    "    print(f\"If you are     using Anaconda Navigator, you can upgrade with:  conda update conda, conda update scikit-learn\")\n",
    "    print(f\"If you are not using Anaconda Navigator, you can upgrade with:  pip install --upgrade scikit-learn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoostClassifier with multiple base classifiers\n",
    "\n",
    "print(f\"The following base classifiers have already been optimized:\")\n",
    "print('\\n')\n",
    "print(lr_clf)\n",
    "print(nb_clf)\n",
    "print(svm_clf)\n",
    "print(knn_clf)\n",
    "print(mlp_clf)\n",
    "print('\\n')\n",
    "\n",
    "# Define multiple base classifiers\n",
    "#base_classifiers = [\n",
    "#    LogisticRegression(C=100, max_iter=100, penalty='l2', solver='liblinear'),\n",
    "#    BernoulliNB(alpha=0.1),\n",
    "#    SVC(kernel='linear', C=0.1),  # Support Vector Machine with linear kernel\n",
    "#    KNeighborsClassifier(n_neighbors=10, weights='uniform'),\n",
    "#    MLPClassifier(hidden_layer_sizes=[50,25], max_iter=800)\n",
    "#]\n",
    "base_classifiers = [lr_clf, nb_clf, svm_clf, knn_clf, mlp_clf]\n",
    "\n",
    "# Create the AdaBoostClassifier, setting estimator=None because we will add multiple base_classifiers in the next step\n",
    "clf = AdaBoostClassifier(estimator=None, n_estimators=50, random_state=42)\n",
    "\n",
    "# Set the base classifiers as the base estimator\n",
    "clf.estimator_ = base_classifiers\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Final cross validation\n",
    "cross_val_score_result = cross_val_score(clf, X_train_resampled, y_train_label_resampled, cv=cv_count)\n",
    "print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "boosting_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "boosting_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "# Evaluate the model accuracy\n",
    "Accuracy = accuracy_score(y_test_label, y_pred)\n",
    "print(f\"Final Accuracy on Test Set: {Accuracy}\")\n",
    "# This method of calculating accuracy generates an error with AdaBoostClassifier\n",
    "#Accuracy = clf.score(X_test, y_test_label)\n",
    "#print(\"Accuracy:\", Accuracy)\n",
    "#print('\\n')\n",
    "    \n",
    "# call previously defined function to create confusion matrix\n",
    "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "accuracy_ensemble_boosting      = Accuracy\n",
    "sensitivity_ensemble_boosting   = Sensitivity\n",
    "specificity_ensemble_boosting   = Specificity\n",
    "geometricmean_ensemble_boosting = GeometricMean\n",
    "precision_ensemble_boosting     = Precision\n",
    "recall_ensemble_boosting        = Recall\n",
    "f1_ensemble_boosting            = F1\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following cell does not increase the accuracy (actually decreases accuracy by a small amount), \n",
    "# so the following cell has been commented out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Performing hyperparameter optimization for AdaBoostClassifier\")\n",
    "# print(f\"The following base classifiers have already been optimized:\")\n",
    "# print('\\n')\n",
    "# print(lr_clf)\n",
    "# print(nb_clf)\n",
    "# print(svm_clf)\n",
    "# print(knn_clf)\n",
    "# print(mlp_clf)\n",
    "# print('\\n')\n",
    "\n",
    "# # Define multiple base classifiers\n",
    "# #base_classifiers = [\n",
    "# #    LogisticRegression(C=100, max_iter=100, penalty='l2', solver='liblinear'),\n",
    "# #    BernoulliNB(alpha=0.1),\n",
    "# #    SVC(kernel='linear', C=0.1),  # Support Vector Machine with linear kernel\n",
    "# #    KNeighborsClassifier(n_neighbors=10, weights='uniform'),\n",
    "# #    MLPClassifier(hidden_layer_sizes=[50,25], max_iter=800)\n",
    "# #]\n",
    "# base_classifiers = [lr_clf, nb_clf, svm_clf, knn_clf, mlp_clf]\n",
    "\n",
    "# # Define the hyperparameters to tune for AdaBoostClassifier\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 200],               # Number of boosting rounds\n",
    "#     'learning_rate': [0.01, 0.1, 1.0]             # Weight applied to each classifier\n",
    "# }\n",
    "\n",
    "# # Create the AdaBoostClassifier, setting estimator=None because we will add multiple base_classifiers in the next step\n",
    "# clf = AdaBoostClassifier(estimator=None, random_state=42)\n",
    "\n",
    "# # Set the base classifiers as the base_estimator\n",
    "# clf.estimator_ = base_classifiers\n",
    "\n",
    "# # Use GridSearchCV for hyperparameter tuning\n",
    "# print(f\"Performing GridSearchCV\")\n",
    "# grid_search = GridSearchCV(clf, param_grid, cv=cv_count, scoring='accuracy')\n",
    "# print(f\"Fitting model\")\n",
    "# grid_search.fit(X_train_resampled, y_train_label_resampled)\n",
    "\n",
    "# # Validate on Test Set\n",
    "# clf = grid_search.best_estimator_\n",
    "# print(f\"Found best_estimator_  {clf}\")\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "# # Final cross validation\n",
    "# cross_val_score_result = cross_val_score(clf, X_train_resampled, y_train_label_resampled, cv=cv_count)\n",
    "# print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "# print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "# print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "# boosting_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "# boosting_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "\n",
    "# # Evaluate the model accuracy\n",
    "# Accuracy = accuracy_score(y_test_label, y_pred)\n",
    "# print(f\"Final Accuracy on Test Set: {Accuracy}\")\n",
    "# # This method of calculating accuracy generates an error with AdaBoostClassifier\n",
    "# #Accuracy = clf.score(X_test, y_test_label)\n",
    "# #print(\"Accuracy:\", Accuracy)\n",
    "# #print('\\n')\n",
    "    \n",
    "# # call previously defined function to create confusion matrix\n",
    "# cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test_label, y_pred)\n",
    "\n",
    "# # save results calculated for this model for later comparison to other models\n",
    "# accuracy_ensemble_boosting      = Accuracy\n",
    "# sensitivity_ensemble_boosting   = Sensitivity\n",
    "# specificity_ensemble_boosting   = Specificity\n",
    "# geometricmean_ensemble_boosting = GeometricMean\n",
    "# precision_ensemble_boosting     = Precision\n",
    "# recall_ensemble_boosting        = Recall\n",
    "# f1_ensemble_boosting            = F1\n",
    "\n",
    "# # show a running total of elapsed time for the entire notebook\n",
    "# show_elapsed_time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LR  accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_lr_undersampled_unoptimized*100:.2f}%\")\n",
    "print(f\"LR  accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_lr_undersampled_optimized*100:.2f}%\")\n",
    "print('\\n')\n",
    "print(f\"DT  accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_dt_undersampled_unoptimized*100:.2f}%\")\n",
    "print(f\"DT  accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_dt_undersampled_optimized*100:.2f}%\")\n",
    "print('\\n')\n",
    "print(f\"DS  accuracy on undersampled balanced data, without hyperparameter optimimization: {accuracy_ds_undersampled_unoptimized*100:.2f}%\")\n",
    "print(f\"DS  accuracy on undersampled balanced data, with    hyperparameter optimimization: {accuracy_ds_undersampled_optimized*100:.2f}%\")\n",
    "print('\\n')\n",
    "print(f\"RF  accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_rf_undersampled_unoptimized*100:.2f}%\")\n",
    "print(f\"RF  accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_rf_undersampled_optimized*100:.2f}%\")\n",
    "print('\\n')\n",
    "print(f\"NB  accuracy on undersampled balanced data, without hyperparameter optimimization: {accuracy_nb_undersampled_unoptimized*100:.2f}%\")\n",
    "print(f\"NB  accuracy on undersampled balanced data, with    hyperparameter optimimization: {accuracy_nb_undersampled_optimized*100:.2f}%\")\n",
    "print('\\n')\n",
    "print(f\"SVM accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_svm_undersampled_unoptimized*100:.2f}%\")\n",
    "print(f\"SVM accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_svm_undersampled_optimized*100:.2f}%\")\n",
    "print('\\n')\n",
    "print(f\"KNN accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_knn_undersampled_unoptimized*100:.2f}%\")\n",
    "print(f\"KNN accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_knn_undersampled_optimized*100:.2f}%\")\n",
    "print('\\n')\n",
    "print(f\"MLP accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_mlp_undersampled_unoptimized*100:.2f}%\")\n",
    "print(f\"MLP accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_mlp_undersampled_optimized*100:.2f}%\")\n",
    "print('\\n')\n",
    "print(f\"GB  accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_gb_undersampled_unoptimized*100:.2f}%\")\n",
    "print(f\"GB  accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_gb_undersampled_optimized*100:.2f}%\")\n",
    "print('\\n')\n",
    "print(f\"XGB accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_xgb_undersampled_unoptimized*100:.2f}%\")\n",
    "print(f\"XGB accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_xgb_undersampled_optimized*100:.2f}%\")\n",
    "print('\\n')\n",
    "print(f\"Ensemble voting   accuracy on undersampled balanced data, after hyperparameter optimimization: {accuracy_ensemble_voting*100:.2f}%\")\n",
    "print(f\"Ensemble stacking accuracy on undersampled balanced data, after hyperparameter optimimization: {accuracy_ensemble_stacking*100:.2f}%\")\n",
    "print(f\"Ensemble bagging  accuracy on undersampled balanced data, after hyperparameter optimimization: {accuracy_ensemble_bagging*100:.2f}%\")\n",
    "print(f\"Ensemble boosting accuracy on undersampled balanced data, after hyperparameter optimimization: {accuracy_ensemble_boosting*100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy_lr_undersampled_optimized = 0.8740\n",
    "#accuracy_nb_undersampled_optimized = 0.7679\n",
    "#accuracy_svm_undersampled_optimized = 0.8777\n",
    "#accuracy_knn_undersampled_optimized = 0.8803\n",
    "#accuracy_mlp_undersampled_optimized = 0.8884\n",
    "#accuracy_ensemble_voting = 0.8856\n",
    "#accuracy_ensemble_stacking = 0.8930\n",
    "#accuracy_ensemble_bagging = 0.9115\n",
    "#accuracy_ensemble_boosting = 0.9458"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar graph that shows the accuracy of the base classifiers and ensemble classifiers\n",
    "\n",
    "# Show the values that will be used in the graph\n",
    "print(f\"The following accuracy values will be used for visualization:\")\n",
    "print(f\"   LR       {accuracy_lr_undersampled_optimized:.4f}\")\n",
    "print(f\"   NB       {accuracy_nb_undersampled_optimized:.4f}\")\n",
    "print(f\"   SVM      {accuracy_svm_undersampled_optimized:.4f}\")\n",
    "print(f\"   KNN      {accuracy_knn_undersampled_optimized:.4f}\")\n",
    "print(f\"   MLP      {accuracy_mlp_undersampled_optimized:.4f}\")\n",
    "print(f\"   Voting   {accuracy_ensemble_voting:.4f}\")\n",
    "print(f\"   Stacking {accuracy_ensemble_stacking:.4f}\")\n",
    "print(f\"   Bagging  {accuracy_ensemble_bagging:.4f}\")\n",
    "print(f\"   Boosting {accuracy_ensemble_boosting:.4f}\")\n",
    "\n",
    "labels = [\"LR\", \"NB\", \"SVM\", \"KNN\", \"MLP\", \"Voting\", \"Stacking\", \"Bagging\", \"Boosting\"]\n",
    "values = [accuracy_lr_undersampled_optimized*100, accuracy_nb_undersampled_optimized*100, accuracy_svm_undersampled_optimized*100, accuracy_knn_undersampled_optimized*100, accuracy_mlp_undersampled_optimized*100, accuracy_ensemble_voting*100, accuracy_ensemble_stacking*100, accuracy_ensemble_bagging*100, accuracy_ensemble_boosting*100]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Increase the width of the graph\n",
    "fig, ax = plt.subplots(figsize=(10, 6))  # Adjust the figsize as needed\n",
    "\n",
    "# Increase spacing between bars\n",
    "bar_width = 0.6  # Adjust the width as needed\n",
    "bar_positions = range(len(labels))\n",
    "\n",
    "# Create a bar graph\n",
    "#bars = plt.bar(bar_positions, values, width=bar_width, color='blue')\n",
    "bars = plt.bar(bar_positions, values, width=bar_width, color=['lightgreen']*5 + ['darkgreen']*4)  # Last 4 bars are darkgreen\n",
    "\n",
    "# Dynamically set y-axis limits\n",
    "plt.ylim(min(values*100) - 5, max(values) + 5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Model Accuracies for Edge-IIoTset2023 dataset')\n",
    "\n",
    "# Set x-axis ticks and labels\n",
    "plt.xticks(bar_positions, labels)\n",
    "\n",
    "# Annotate each bar with its respective value\n",
    "for bar, value in zip(bars, values):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1, f'{value:.2f}%', ha='center', va='bottom')\n",
    "\n",
    "# Display the bar graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as previous cell but a horizontal bar graph instead of vertical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Show the values that will be used in the graph\n",
    "print(f\"The following accuracy values will be used for visualization:\")\n",
    "print(f\"   LR       {accuracy_lr_undersampled_optimized:.4f}\")\n",
    "print(f\"   NB       {accuracy_nb_undersampled_optimized:.4f}\")\n",
    "print(f\"   SVM      {accuracy_svm_undersampled_optimized:.4f}\")\n",
    "print(f\"   KNN      {accuracy_knn_undersampled_optimized:.4f}\")\n",
    "print(f\"   MLP      {accuracy_mlp_undersampled_optimized:.4f}\")\n",
    "print(f\"   Voting   {accuracy_ensemble_voting:.4f}\")\n",
    "print(f\"   Stacking {accuracy_ensemble_stacking:.4f}\")\n",
    "print(f\"   Bagging  {accuracy_ensemble_bagging:.4f}\")\n",
    "print(f\"   Boosting {accuracy_ensemble_boosting:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Given data\n",
    "labels = [\"LR\", \"NB\", \"SVM\", \"KNN\", \"MLP\", \"Voting\", \"Stacking\", \"Bagging\", \"Boosting\"]\n",
    "values = [accuracy_lr_undersampled_optimized*100, accuracy_nb_undersampled_optimized*100, accuracy_svm_undersampled_optimized*100, accuracy_knn_undersampled_optimized*100, accuracy_mlp_undersampled_optimized*100, accuracy_ensemble_voting*100, accuracy_ensemble_stacking*100, accuracy_ensemble_bagging*100, accuracy_ensemble_boosting*100]\n",
    "\n",
    "# Increase the height of the graph\n",
    "fig, ax = plt.subplots(figsize=(10, 6))  # Adjust the figsize as needed\n",
    "\n",
    "# Increase spacing between bars\n",
    "bar_height = 0.6  # Adjust the height as needed\n",
    "bar_positions = range(len(labels))\n",
    "\n",
    "# Create a horizontal bar graph\n",
    "#bars = plt.barh(bar_positions, values, height=bar_height, color='blue')\n",
    "bars = plt.barh(bar_positions, values, height=bar_height, color=['lightgreen']*5 + ['darkgreen']*4)  # Last 4 bars are darkgreen\n",
    "\n",
    "# Dynamically set x-axis limits\n",
    "plt.xlim(min(values) - 5, max(values) + 5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.ylabel('')\n",
    "plt.xlabel('Accuracy (%)')\n",
    "plt.title('Model Accuracies for Edge-IIoTset2023 dataset')\n",
    "\n",
    "# Set y-axis ticks and labels\n",
    "plt.yticks(bar_positions, labels)\n",
    "\n",
    "# Annotate each bar with its respective value\n",
    "for bar, value in zip(bars, values):\n",
    "    plt.text(bar.get_width() + 1, bar.get_y() + bar.get_height() / 2, f'{value:.2f}%', va='center', ha='left')\n",
    "\n",
    "# Display the horizontal bar graph\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code generates a box plot with the given mean and standard deviation values for each classifier. \n",
    "# The showmeans=True option ensures that the means are displayed as red dots, and sym='' removes the outliers for a cleaner representation. \n",
    "# Adjust the code as needed for your specific requirements.\n",
    "\n",
    "# Show the values that will be used in the graph\n",
    "print(f\"The following Mean and Standard Deviation values will be used for visualization:\")\n",
    "print(f\"   LR       {lr_crossval_score_mean:.4f}   {lr_crossval_score_std:.4f}\")\n",
    "print(f\"   NB       {nb_crossval_score_mean:.4f}   {nb_crossval_score_std:.4f}\")\n",
    "print(f\"   SVM      {svm_crossval_score_mean:.4f}   {svm_crossval_score_std:.4f}\")\n",
    "print(f\"   KNN      {knn_crossval_score_mean:.4f}   {knn_crossval_score_std:.4f}\")\n",
    "print(f\"   MLP      {mlp_crossval_score_mean:.4f}   {mlp_crossval_score_std:.4f}\")\n",
    "print(f\"   Voting   {voting_crossval_score_mean:.4f}   {voting_crossval_score_std:.4f}\")\n",
    "print(f\"   Stacking {stacking_crossval_score_mean:.4f}   {stacking_crossval_score_std:.4f}\")\n",
    "print(f\"   Bagging  {bagging_crossval_score_mean:.4f}   {bagging_crossval_score_std:.4f}\")\n",
    "print(f\"   Boosting {boosting_crossval_score_mean:.4f}   {boosting_crossval_score_std:.4f}\")\n",
    "\n",
    "# Prepare the data\n",
    "labels = [\"LR\", \"NB\", \"SVM\", \"KNN\", \"MLP\", \"Voting\", \"Stacking\", \"Bagging\", \"Boosting\"]\n",
    "means = [lr_crossval_score_mean*100, nb_crossval_score_mean*100, svm_crossval_score_mean*100, knn_crossval_score_mean*100, mlp_crossval_score_mean*100, voting_crossval_score_mean*100, stacking_crossval_score_mean*100, bagging_crossval_score_mean*100, boosting_crossval_score_mean*100]\n",
    "std_devs = [lr_crossval_score_std*100, nb_crossval_score_std*100, svm_crossval_score_std*100, knn_crossval_score_std*100, mlp_crossval_score_std*100, voting_crossval_score_std*100, stacking_crossval_score_std*100, bagging_crossval_score_std*100, boosting_crossval_score_std*100]\n",
    "\n",
    "# Create the box plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the box plots\n",
    "box_data = [np.random.normal(mean, std, 100) for mean, std in zip(means, std_devs)]\n",
    "ax.boxplot(box_data, labels=labels, showmeans=True, meanline=True, sym='')\n",
    "\n",
    "# Rotate the x-axis labels by 45 degrees to make them fit\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Classifiers')\n",
    "ax.set_ylabel('Mean Accuracy % with Std-Dev')\n",
    "ax.set_title('Classifier Performance for Edge-IIoT2023 dataset')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a graph showing accuracy and F1-score\n",
    "\n",
    "# The fig, ax1 = plt.subplots() creates a figure with two subplots, where ax1 is the left subplot.\n",
    "# The bar graph for accuracy is plotted on the left subplot (ax1), and the line graph for F1-scores is plotted on the right subplot (ax2).\n",
    "# The right y-axis is shared between the two subplots.\n",
    "\n",
    "# Show the values that will be used in the graph\n",
    "print(f\"The following Accuracy and F1-Score values will be used for visualization:\")\n",
    "print(f\"   LR       Accuracy:{accuracy_lr_undersampled_optimized:.4f}   F1-Score:{f1_lr_undersampled_optimized:.4f}\")\n",
    "print(f\"   NB       Accuracy:{accuracy_nb_undersampled_optimized:.4f}   F1-Score:{f1_nb_undersampled_optimized:.4f}\")\n",
    "print(f\"   SVM      Accuracy:{accuracy_svm_undersampled_optimized:.4f}   F1-Score:{f1_svm_undersampled_optimized:.4f}\")\n",
    "print(f\"   KNN      Accuracy:{accuracy_knn_undersampled_optimized:.4f}   F1-Score:{f1_knn_undersampled_optimized:.4f}\")\n",
    "print(f\"   MLP      Accuracy:{accuracy_mlp_undersampled_optimized:.4f}   F1-Score:{f1_mlp_undersampled_optimized:.4f}\")\n",
    "print(f\"   Voting   Accuracy:{accuracy_ensemble_voting:.4f}   F1-Score:{f1_ensemble_voting:.4f}\")\n",
    "print(f\"   Stacking Accuracy:{accuracy_ensemble_stacking:.4f}   F1-Score:{f1_ensemble_stacking:.4f}\")\n",
    "print(f\"   Bagging  Accuracy:{accuracy_ensemble_bagging:.4f}   F1-Score:{f1_ensemble_bagging:.4f}\")\n",
    "print(f\"   Boosting Accuracy:{accuracy_ensemble_boosting:.4f}   F1-Score:{f1_ensemble_boosting:.4f}\")\n",
    "\n",
    "\n",
    "# Prepare the data\n",
    "labels = [\"LR\", \"NB\", \"SVM\", \"KNN\",\"MLP\", \"Voting\", \"Stacking\", \"Bagging\", \"Boosting\"]\n",
    "accuracy = [accuracy_lr_undersampled_optimized*100, accuracy_nb_undersampled_optimized*100, accuracy_svm_undersampled_optimized*100, accuracy_knn_undersampled_optimized*100, accuracy_mlp_undersampled_optimized*100, accuracy_ensemble_voting*100, accuracy_ensemble_stacking*100, accuracy_ensemble_bagging*100, accuracy_ensemble_boosting*100]\n",
    "f1score = [f1_lr_undersampled_optimized*100, f1_nb_undersampled_optimized*100, f1_svm_undersampled_optimized*100, f1_knn_undersampled_optimized*100, f1_mlp_undersampled_optimized*100, f1_ensemble_voting*100, f1_ensemble_stacking*100, f1_ensemble_bagging*100, f1_ensemble_boosting*100]\n",
    "                                             \n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Bar graph for accuracy on the left y-axis\n",
    "color = 'skyblue'\n",
    "ax1.bar(labels, accuracy, color=color, label='Accuracy')\n",
    "ax1.set_xlabel('Classifiers')\n",
    "ax1.set_ylabel('Accuracy', color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.set_title('Classifier Accuracy and F1 Score for Edge-IIoT2023 dataset')\n",
    "\n",
    "# Line graph for F1-scores on the right y-axis\n",
    "ax2 = ax1.twinx()\n",
    "color = 'orange'\n",
    "ax2.plot(labels, f1score, marker='o', color=color, label='F1 Score')\n",
    "ax2.set_ylabel('F1 Score', color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "## Set the same y-axis limits for both axes\n",
    "min_y = min(min(accuracy), min(f1score))\n",
    "max_y = max(max(accuracy), max(f1score))\n",
    "ax1.set_ylim(min_y-5, max_y+5)\n",
    "ax2.set_ylim(min_y-5, max_y+5)\n",
    "\n",
    "\n",
    "# Show legends for both graphs\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a graph showing multiple lines\n",
    "\n",
    "# Multiple lines are plotted for each performance metric (accuracy, sensitivity, specificity, geometric mean, precision, recall, F1 score).\n",
    "# plt.legend() is used to display a legend for better identification of each line.\n",
    "# The x-axis labels are rotated by 45 degrees (plt.xticks(rotation=45, ha='right')) for better visibility.\n",
    "\n",
    "# Show the values that will be used in the graph\n",
    "print(f\"The following values will be used for visualization:\")\n",
    "print(f\"   LR       Acc:{accuracy_lr_undersampled_optimized:.4f}   Sens:{sensitivity_lr_undersampled_optimized:.4f}  Spec:{specificity_lr_undersampled_optimized:.4f}  GM:{geometricmean_lr_undersampled_optimized:.4f}  Prec:{precision_lr_undersampled_optimized:.4f}  Rec:{recall_lr_undersampled_optimized:.4f}  F1:{f1_lr_undersampled_optimized:.4f}\")\n",
    "print(f\"   NB       Acc:{accuracy_nb_undersampled_optimized:.4f}   Sens:{sensitivity_nb_undersampled_optimized:.4f}  Spec:{specificity_nb_undersampled_optimized:.4f}  GM:{geometricmean_nb_undersampled_optimized:.4f}  Prec:{precision_nb_undersampled_optimized:.4f}  Rec:{recall_nb_undersampled_optimized:.4f}  F1:{f1_nb_undersampled_optimized:.4f}\")\n",
    "print(f\"   SVM      Acc:{accuracy_svm_undersampled_optimized:.4f}   Sens:{sensitivity_svm_undersampled_optimized:.4f}  Spec:{specificity_svm_undersampled_optimized:.4f}  GM:{geometricmean_svm_undersampled_optimized:.4f}  Prec:{precision_svm_undersampled_optimized:.4f}  Rec:{recall_svm_undersampled_optimized:.4f}  F1:{f1_svm_undersampled_optimized:.4f}\")\n",
    "print(f\"   KNN      Acc:{accuracy_knn_undersampled_optimized:.4f}   Sens:{sensitivity_knn_undersampled_optimized:.4f}  Spec:{specificity_knn_undersampled_optimized:.4f}  GM:{geometricmean_knn_undersampled_optimized:.4f}  Prec:{precision_knn_undersampled_optimized:.4f}  Rec:{recall_knn_undersampled_optimized:.4f}  F1:{f1_knn_undersampled_optimized:.4f}\")\n",
    "print(f\"   MLP      Acc:{accuracy_mlp_undersampled_optimized:.4f}   Sens:{sensitivity_mlp_undersampled_optimized:.4f}  Spec:{specificity_mlp_undersampled_optimized:.4f}  GM:{geometricmean_mlp_undersampled_optimized:.4f}  Prec:{precision_mlp_undersampled_optimized:.4f}  Rec:{recall_mlp_undersampled_optimized:.4f}  F1:{f1_mlp_undersampled_optimized:.4f}\")\n",
    "print(f\"   Voting   Acc:{accuracy_ensemble_voting:.4f}   Sens:{sensitivity_ensemble_voting:.4f}  Spec:{sensitivity_ensemble_voting:.4f}  GM:{geometricmean_ensemble_voting:.4f}  Prec:{precision_ensemble_voting:.4f}  Rec:{recall_ensemble_voting:.4f}  F1:{f1_ensemble_voting:.4f}\")\n",
    "print(f\"   Stacking Acc:{accuracy_ensemble_stacking:.4f}   Sens:{sensitivity_ensemble_stacking:.4f}  Spec:{sensitivity_ensemble_stacking:.4f}  GM:{geometricmean_ensemble_stacking:.4f}  Prec:{precision_ensemble_stacking:.4f}  Rec:{recall_ensemble_stacking:.4f}  F1:{f1_ensemble_stacking:.4f}\")\n",
    "print(f\"   Bagging  Acc:{accuracy_ensemble_bagging:.4f}   Sens:{sensitivity_ensemble_bagging:.4f}  Spec:{sensitivity_ensemble_bagging:.4f}  GM:{geometricmean_ensemble_bagging:.4f}  Prec:{precision_ensemble_bagging:.4f}  Rec:{recall_ensemble_bagging:.4f}  F1:{f1_ensemble_bagging:.4f}\")\n",
    "print(f\"   Boosting Acc:{accuracy_ensemble_boosting:.4f}   Sens:{sensitivity_ensemble_boosting:.4f}  Spec:{sensitivity_ensemble_boosting:.4f}  GM:{geometricmean_ensemble_boosting:.4f}  Prec:{precision_ensemble_boosting:.4f}  Rec:{recall_ensemble_boosting:.4f}  F1:{f1_ensemble_boosting:.4f}\")\n",
    "\n",
    "\n",
    "# Prepare the data\n",
    "labels = [\"LR\", \"NB\", \"SVM\", \"KNN\",\"MLP\", \"Voting\", \"Stacking\", \"Bagging\", \"Boosting\"]\n",
    "accuracy = [accuracy_lr_undersampled_optimized*100, accuracy_nb_undersampled_optimized*100, accuracy_svm_undersampled_optimized*100, accuracy_knn_undersampled_optimized*100, accuracy_mlp_undersampled_optimized*100, accuracy_ensemble_voting*100, accuracy_ensemble_stacking*100, accuracy_ensemble_bagging*100, accuracy_ensemble_boosting*100]\n",
    "sensitivity = [sensitivity_lr_undersampled_optimized*100, sensitivity_nb_undersampled_optimized*100, sensitivity_svm_undersampled_optimized*100, sensitivity_knn_undersampled_optimized*100, sensitivity_mlp_undersampled_optimized*100, sensitivity_ensemble_voting*100, sensitivity_ensemble_stacking*100, sensitivity_ensemble_bagging*100, sensitivity_ensemble_boosting*100]\n",
    "specificity = [specificity_lr_undersampled_optimized*100, specificity_nb_undersampled_optimized*100, specificity_svm_undersampled_optimized*100, specificity_knn_undersampled_optimized*100, specificity_mlp_undersampled_optimized*100, specificity_ensemble_voting*100, specificity_ensemble_stacking*100, specificity_ensemble_bagging*100, specificity_ensemble_boosting*100]\n",
    "geometricmean = [geometricmean_lr_undersampled_optimized*100, geometricmean_nb_undersampled_optimized*100, geometricmean_svm_undersampled_optimized*100, geometricmean_knn_undersampled_optimized*100, geometricmean_mlp_undersampled_optimized*100, geometricmean_ensemble_voting*100, geometricmean_ensemble_stacking*100, geometricmean_ensemble_bagging*100, geometricmean_ensemble_boosting*100]\n",
    "precision = [precision_lr_undersampled_optimized*100, precision_nb_undersampled_optimized*100, precision_svm_undersampled_optimized*100, precision_knn_undersampled_optimized*100, precision_mlp_undersampled_optimized*100, precision_ensemble_voting*100, precision_ensemble_stacking*100, precision_ensemble_bagging*100, precision_ensemble_boosting*100]\n",
    "recall = [recall_lr_undersampled_optimized*100, recall_nb_undersampled_optimized*100, recall_svm_undersampled_optimized*100, recall_knn_undersampled_optimized*100, recall_mlp_undersampled_optimized*100, recall_ensemble_voting*100, recall_ensemble_stacking*100, recall_ensemble_bagging*100, recall_ensemble_boosting*100]\n",
    "f1score = [f1_lr_undersampled_optimized*100, f1_nb_undersampled_optimized*100, f1_svm_undersampled_optimized*100, f1_knn_undersampled_optimized*100, f1_mlp_undersampled_optimized*100, f1_ensemble_voting*100, f1_ensemble_stacking*100, f1_ensemble_bagging*100, f1_ensemble_boosting*100]\n",
    "\n",
    "# Create a line graph with multiple lines\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot lines for each metric\n",
    "plt.plot(labels, accuracy, label='Accuracy', marker='o')\n",
    "plt.plot(labels, sensitivity, label='Sensitivity', marker='o')\n",
    "plt.plot(labels, specificity, label='Specificity', marker='o')\n",
    "plt.plot(labels, geometricmean, label='Geometric Mean', marker='o')\n",
    "plt.plot(labels, precision, label='Precision', marker='o')\n",
    "plt.plot(labels, recall, label='Recall', marker='o')\n",
    "plt.plot(labels, f1score, label='F1 Score', marker='o')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Classifiers')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.title('Performance Metrics for Classifiers for Edge-IIoT2023 dataset')\n",
    "plt.legend()  # Show legend\n",
    "\n",
    "# Rotate x-axis labels for better visibility\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to previous graph, but only shows Accuracy, Sensitivity, Specificity, Geometric Mean\n",
    "\n",
    "# create a graph showing multiple lines\n",
    "\n",
    "# Multiple lines are plotted for each performance metric (accuracy, sensitivity, specificity, geometric mean, precision, recall, F1 score).\n",
    "# plt.legend() is used to display a legend for better identification of each line.\n",
    "# The x-axis labels are rotated by 45 degrees (plt.xticks(rotation=45, ha='right')) for better visibility.\n",
    "\n",
    "# Show the values that will be used in the graph\n",
    "print(f\"The following values will be used for visualization:\")\n",
    "print(f\"   LR       Acc:{accuracy_lr_undersampled_optimized:.4f}   Sens:{sensitivity_lr_undersampled_optimized:.4f}  Spec:{specificity_lr_undersampled_optimized:.4f}  GM:{geometricmean_lr_undersampled_optimized:.4f}\")\n",
    "print(f\"   NB       Acc:{accuracy_nb_undersampled_optimized:.4f}   Sens:{sensitivity_nb_undersampled_optimized:.4f}  Spec:{specificity_nb_undersampled_optimized:.4f}  GM:{geometricmean_nb_undersampled_optimized:.4f}\")\n",
    "print(f\"   SVM      Acc:{accuracy_svm_undersampled_optimized:.4f}   Sens:{sensitivity_svm_undersampled_optimized:.4f}  Spec:{specificity_svm_undersampled_optimized:.4f}  GM:{geometricmean_svm_undersampled_optimized:.4f}\")\n",
    "print(f\"   KNN      Acc:{accuracy_knn_undersampled_optimized:.4f}   Sens:{sensitivity_knn_undersampled_optimized:.4f}  Spec:{specificity_knn_undersampled_optimized:.4f}  GM:{geometricmean_knn_undersampled_optimized:.4f}\")\n",
    "print(f\"   MLP      Acc:{accuracy_mlp_undersampled_optimized:.4f}   Sens:{sensitivity_mlp_undersampled_optimized:.4f}  Spec:{specificity_mlp_undersampled_optimized:.4f}  GM:{geometricmean_mlp_undersampled_optimized:.4f}\")\n",
    "print(f\"   Voting   Acc:{accuracy_ensemble_voting:.4f}   Sens:{sensitivity_ensemble_voting:.4f}  Spec:{sensitivity_ensemble_voting:.4f}  GM:{geometricmean_ensemble_voting:.4f}\")\n",
    "print(f\"   Stacking Acc:{accuracy_ensemble_stacking:.4f}   Sens:{sensitivity_ensemble_stacking:.4f}  Spec:{sensitivity_ensemble_stacking:.4f}  GM:{geometricmean_ensemble_stacking:.4f}\")\n",
    "print(f\"   Bagging  Acc:{accuracy_ensemble_bagging:.4f}   Sens:{sensitivity_ensemble_bagging:.4f}  Spec:{sensitivity_ensemble_bagging:.4f}  GM:{geometricmean_ensemble_bagging:.4f}\")\n",
    "print(f\"   Boosting Acc:{accuracy_ensemble_boosting:.4f}   Sens:{sensitivity_ensemble_boosting:.4f}  Spec:{sensitivity_ensemble_boosting:.4f}  GM:{geometricmean_ensemble_boosting:.4f}\")\n",
    "\n",
    "\n",
    "# Prepare the data\n",
    "labels = [\"LR\", \"NB\", \"SVM\", \"KNN\",\"MLP\", \"Voting\", \"Stacking\", \"Bagging\", \"Boosting\"]\n",
    "accuracy = [accuracy_lr_undersampled_optimized*100, accuracy_nb_undersampled_optimized*100, accuracy_svm_undersampled_optimized*100, accuracy_knn_undersampled_optimized*100, accuracy_mlp_undersampled_optimized*100, accuracy_ensemble_voting*100, accuracy_ensemble_stacking*100, accuracy_ensemble_bagging*100, accuracy_ensemble_boosting*100]\n",
    "sensitivity = [sensitivity_lr_undersampled_optimized*100, sensitivity_nb_undersampled_optimized*100, sensitivity_svm_undersampled_optimized*100, sensitivity_knn_undersampled_optimized*100, sensitivity_mlp_undersampled_optimized*100, sensitivity_ensemble_voting*100, sensitivity_ensemble_stacking*100, sensitivity_ensemble_bagging*100, sensitivity_ensemble_boosting*100]\n",
    "specificity = [specificity_lr_undersampled_optimized*100, specificity_nb_undersampled_optimized*100, specificity_svm_undersampled_optimized*100, specificity_knn_undersampled_optimized*100, specificity_mlp_undersampled_optimized*100, specificity_ensemble_voting*100, specificity_ensemble_stacking*100, specificity_ensemble_bagging*100, specificity_ensemble_boosting*100]\n",
    "geometricmean = [geometricmean_lr_undersampled_optimized*100, geometricmean_nb_undersampled_optimized*100, geometricmean_svm_undersampled_optimized*100, geometricmean_knn_undersampled_optimized*100, geometricmean_mlp_undersampled_optimized*100, geometricmean_ensemble_voting*100, geometricmean_ensemble_stacking*100, geometricmean_ensemble_bagging*100, geometricmean_ensemble_boosting*100]\n",
    "precision = [precision_lr_undersampled_optimized*100, precision_nb_undersampled_optimized*100, precision_svm_undersampled_optimized*100, precision_knn_undersampled_optimized*100, precision_mlp_undersampled_optimized*100, precision_ensemble_voting*100, precision_ensemble_stacking*100, precision_ensemble_bagging*100, precision_ensemble_boosting*100]\n",
    "recall = [recall_lr_undersampled_optimized*100, recall_nb_undersampled_optimized*100, recall_svm_undersampled_optimized*100, recall_knn_undersampled_optimized*100, recall_mlp_undersampled_optimized*100, recall_ensemble_voting*100, recall_ensemble_stacking*100, recall_ensemble_bagging*100, recall_ensemble_boosting*100]\n",
    "f1score = [f1_lr_undersampled_optimized*100, f1_nb_undersampled_optimized*100, f1_svm_undersampled_optimized*100, f1_knn_undersampled_optimized*100, f1_mlp_undersampled_optimized*100, f1_ensemble_voting*100, f1_ensemble_stacking*100, f1_ensemble_bagging*100, f1_ensemble_boosting*100]\n",
    "\n",
    "# Create a line graph with multiple lines\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot lines for each metric\n",
    "plt.plot(labels, accuracy, label='Accuracy', marker='o')\n",
    "plt.plot(labels, sensitivity, label='Sensitivity', marker='o')\n",
    "plt.plot(labels, specificity, label='Specificity', marker='o')\n",
    "plt.plot(labels, geometricmean, label='Geometric Mean', marker='o')\n",
    "#plt.plot(labels, precision, label='Precision', marker='o')\n",
    "#plt.plot(labels, recall, label='Recall', marker='o')\n",
    "#plt.plot(labels, f1score, label='F1 Score', marker='o')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Classifiers')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.title('Performance Metrics for Classifiers for Edge-IIoT2023 dataset')\n",
    "plt.legend()  # Show legend\n",
    "\n",
    "# Rotate x-axis labels for better visibility\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to previous graph, but only shows Accuracy, Precision, Recall, F1-score\n",
    "\n",
    "# create a graph showing multiple lines\n",
    "\n",
    "# Multiple lines are plotted for each performance metric (accuracy, sensitivity, specificity, geometric mean, precision, recall, F1 score).\n",
    "# plt.legend() is used to display a legend for better identification of each line.\n",
    "# The x-axis labels are rotated by 45 degrees (plt.xticks(rotation=45, ha='right')) for better visibility.\n",
    "\n",
    "\n",
    "# Show the values that will be used in the graph\n",
    "print(f\"The following values will be used for visualization:\")\n",
    "print(f\"   LR       Acc:{accuracy_lr_undersampled_optimized:.4f}   Prec:{precision_lr_undersampled_optimized:.4f}  Rec:{recall_lr_undersampled_optimized:.4f}  F1:{f1_lr_undersampled_optimized:.4f}\")\n",
    "print(f\"   NB       Acc:{accuracy_nb_undersampled_optimized:.4f}   Prec:{precision_nb_undersampled_optimized:.4f}  Rec:{recall_nb_undersampled_optimized:.4f}  F1:{f1_nb_undersampled_optimized:.4f}\")\n",
    "print(f\"   SVM      Acc:{accuracy_svm_undersampled_optimized:.4f}   Prec:{precision_svm_undersampled_optimized:.4f}  Rec:{recall_svm_undersampled_optimized:.4f}  F1:{f1_svm_undersampled_optimized:.4f}\")\n",
    "print(f\"   KNN      Acc:{accuracy_knn_undersampled_optimized:.4f}   Prec:{precision_knn_undersampled_optimized:.4f}  Rec:{recall_knn_undersampled_optimized:.4f}  F1:{f1_knn_undersampled_optimized:.4f}\")\n",
    "print(f\"   MLP      Acc:{accuracy_mlp_undersampled_optimized:.4f}   Prec:{precision_mlp_undersampled_optimized:.4f}  Rec:{recall_mlp_undersampled_optimized:.4f}  F1:{f1_mlp_undersampled_optimized:.4f}\")\n",
    "print(f\"   Voting   Acc:{accuracy_ensemble_voting:.4f}   Prec:{precision_ensemble_voting:.4f}  Rec:{recall_ensemble_voting:.4f}  F1:{f1_ensemble_voting:.4f}\")\n",
    "print(f\"   Stacking Acc:{accuracy_ensemble_stacking:.4f}   Prec:{precision_ensemble_stacking:.4f}  Rec:{recall_ensemble_stacking:.4f}  F1:{f1_ensemble_stacking:.4f}\")\n",
    "print(f\"   Bagging  Acc:{accuracy_ensemble_bagging:.4f}   Prec:{precision_ensemble_bagging:.4f}  Rec:{recall_ensemble_bagging:.4f}  F1:{f1_ensemble_bagging:.4f}\")\n",
    "print(f\"   Boosting Acc:{accuracy_ensemble_boosting:.4f}   Prec:{precision_ensemble_boosting:.4f}  Rec:{recall_ensemble_boosting:.4f}  F1:{f1_ensemble_boosting:.4f}\")\n",
    "\n",
    "\n",
    "# Prepare the data\n",
    "labels = [\"LR\", \"NB\", \"SVM\", \"KNN\",\"MLP\", \"Voting\", \"Stacking\", \"Bagging\", \"Boosting\"]\n",
    "accuracy = [accuracy_lr_undersampled_optimized*100, accuracy_nb_undersampled_optimized*100, accuracy_svm_undersampled_optimized*100, accuracy_knn_undersampled_optimized*100, accuracy_mlp_undersampled_optimized*100, accuracy_ensemble_voting*100, accuracy_ensemble_stacking*100, accuracy_ensemble_bagging*100, accuracy_ensemble_boosting*100]\n",
    "sensitivity = [sensitivity_lr_undersampled_optimized*100, sensitivity_nb_undersampled_optimized*100, sensitivity_svm_undersampled_optimized*100, sensitivity_knn_undersampled_optimized*100, sensitivity_mlp_undersampled_optimized*100, sensitivity_ensemble_voting*100, sensitivity_ensemble_stacking*100, sensitivity_ensemble_bagging*100, sensitivity_ensemble_boosting*100]\n",
    "specificity = [specificity_lr_undersampled_optimized*100, specificity_nb_undersampled_optimized*100, specificity_svm_undersampled_optimized*100, specificity_knn_undersampled_optimized*100, specificity_mlp_undersampled_optimized*100, specificity_ensemble_voting*100, specificity_ensemble_stacking*100, specificity_ensemble_bagging*100, specificity_ensemble_boosting*100]\n",
    "geometricmean = [geometricmean_lr_undersampled_optimized*100, geometricmean_nb_undersampled_optimized*100, geometricmean_svm_undersampled_optimized*100, geometricmean_knn_undersampled_optimized*100, geometricmean_mlp_undersampled_optimized*100, geometricmean_ensemble_voting*100, geometricmean_ensemble_stacking*100, geometricmean_ensemble_bagging*100, geometricmean_ensemble_boosting*100]\n",
    "precision = [precision_lr_undersampled_optimized*100, precision_nb_undersampled_optimized*100, precision_svm_undersampled_optimized*100, precision_knn_undersampled_optimized*100, precision_mlp_undersampled_optimized*100, precision_ensemble_voting*100, precision_ensemble_stacking*100, precision_ensemble_bagging*100, precision_ensemble_boosting*100]\n",
    "recall = [recall_lr_undersampled_optimized*100, recall_nb_undersampled_optimized*100, recall_svm_undersampled_optimized*100, recall_knn_undersampled_optimized*100, recall_mlp_undersampled_optimized*100, recall_ensemble_voting*100, recall_ensemble_stacking*100, recall_ensemble_bagging*100, recall_ensemble_boosting*100]\n",
    "f1score = [f1_lr_undersampled_optimized*100, f1_nb_undersampled_optimized*100, f1_svm_undersampled_optimized*100, f1_knn_undersampled_optimized*100, f1_mlp_undersampled_optimized*100, f1_ensemble_voting*100, f1_ensemble_stacking*100, f1_ensemble_bagging*100, f1_ensemble_boosting*100]\n",
    "\n",
    "# Create a line graph with multiple lines\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot lines for each metric\n",
    "plt.plot(labels, accuracy, label='Accuracy', marker='o')\n",
    "#plt.plot(labels, sensitivity, label='Sensitivity', marker='o')\n",
    "#plt.plot(labels, specificity, label='Specificity', marker='o')\n",
    "#plt.plot(labels, geometricmean, label='Geometric Mean', marker='o')\n",
    "plt.plot(labels, precision, label='Precision', marker='o')\n",
    "plt.plot(labels, recall, label='Recall', marker='o')\n",
    "plt.plot(labels, f1score, label='F1 Score', marker='o')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Classifiers')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.title('Performance Metrics for Classifiers for Edge-IIoT2023 dataset')\n",
    "plt.legend()  # Show legend\n",
    "\n",
    "# Rotate x-axis labels for better visibility\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
